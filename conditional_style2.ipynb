{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import get_rank, init_process_group, destroy_process_group, all_gather, get_world_size\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from glob import glob\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import random\n",
    "from conditionDiffusion.unet import Unet\n",
    "from conditionDiffusion.embedding import ConditionalEmbedding\n",
    "from conditionDiffusion.utils import get_named_beta_schedule\n",
    "from conditionDiffusion.diffusion import GaussianDiffusion\n",
    "from conditionDiffusion.Scheduler import GradualWarmupScheduler\n",
    "from PIL import Image\n",
    "import styleGAN.networks_stylegan2 as stylegan\n",
    "import styleGAN.loss as style_loss\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "print(f\"GPUs used:\\t{torch.cuda.device_count()}\")\n",
    "device = torch.device(\"cuda\", 0)\n",
    "print(f\"Device:\\t\\t{device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_list = ['DWI']\n",
    "params = {'image_size': 256,\n",
    "          'lr': 1e-5,\n",
    "          'beta1': 0.5,\n",
    "          'beta2': 0.999,\n",
    "          'batch_size': 16,\n",
    "          'epochs': 1000,\n",
    "          'n_classes': None,\n",
    "          'data_path': '../../data/2D_MRI/',\n",
    "          'image_count': 5000,\n",
    "          'inch': 3,\n",
    "          'modch': 64,\n",
    "          'outch': 1,\n",
    "          'chmul': [1, 2, 4, 8],\n",
    "          'numres': 2,\n",
    "          'dtype': torch.float32,\n",
    "          'cdim': 10,\n",
    "          'useconv': False,\n",
    "          'droprate': 0.1,\n",
    "          'T': 1000,\n",
    "          'w': 1.8,\n",
    "          'v': 0.3,\n",
    "          'multiplier': 2.5,\n",
    "          'threshold': 0.1,\n",
    "          'ddim': True,\n",
    "          }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 49.17it/s]\n",
      "100%|██████████| 894/894 [00:02<00:00, 395.43it/s]\n"
     ]
    }
   ],
   "source": [
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "\n",
    "def transback(data: Tensor) -> Tensor:\n",
    "    return data / 2 + 0.5\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \"\"\"COCO Custom Dataset compatible with torch.utils.data.DataLoader.\"\"\"\n",
    "\n",
    "    def __init__(self, parmas, images, label):\n",
    "\n",
    "        self.images = images\n",
    "        self.args = parmas\n",
    "        self.label = label\n",
    "\n",
    "    def trans(self, image):\n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomHorizontalFlip(1)\n",
    "            image = transform(image)\n",
    "\n",
    "        if random.random() > 0.5:\n",
    "            transform = transforms.RandomVerticalFlip(1)\n",
    "            image = transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "        label = self.label[index]\n",
    "        image = self.trans(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "\n",
    "image_label = []\n",
    "image_path = []\n",
    "for i in tqdm(range(len(class_list))):\n",
    "    image_list = glob(params['data_path']+class_list[i]+'/*.png')\n",
    "    for j in range(len(image_list)):\n",
    "        image_path.append(image_list[j])\n",
    "        image_label.append(i)\n",
    "\n",
    "train_images = torch.zeros(\n",
    "    (len(image_path), params['inch'], params['image_size'], params['image_size']))\n",
    "for i in tqdm(range(len(image_path))):\n",
    "    train_images[i] = trans(Image.open(image_path[i]).convert(\n",
    "        'RGB').resize((params['image_size'], params['image_size'])))\n",
    "train_dataset = CustomDataset(\n",
    "    params, train_images, F.one_hot(torch.tensor(image_label)))\n",
    "dataloader = DataLoader(\n",
    "    train_dataset, batch_size=params['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generator = stylegan.Generator(\n",
    "    z_dim=256,  # Input latent (Z) dimensionality\n",
    "    # Conditioning label (C) dimensionality (0 = no labels)\n",
    "    c_dim=len(class_list),\n",
    "    w_dim=256,  # Intermediate latent (W) dimensionality\n",
    "    img_resolution=params['image_size'],  # Output resolution\n",
    "    img_channels=3,       # Number of output color channels (3 for RGB)\n",
    ").to(device)\n",
    "\n",
    "discriminator = stylegan.Discriminator(\n",
    "    # Conditioning label (C) dimensionality (0 = no labels)\n",
    "    c_dim=len(class_list),\n",
    "    img_resolution=params['image_size'],  # Input resolution\n",
    "    img_channels=3,       # Number of input color channels (3 for RGB)\n",
    "    architecture='resnet',  # Architecture: 'orig', 'skip', 'resnet'\n",
    "    channel_base=32768,   # Overall multiplier for the number of channels\n",
    "    channel_max=256,      # Maximum number of channels in any layer\n",
    "    num_fp16_res=4,       # Use FP16 for the 4 highest resolutions\n",
    "    # Clamp the output of convolution layers to +-X, None = disable clamping\n",
    "    conv_clamp=None,\n",
    "    cmap_dim=None,        # Dimensionality of mapped conditioning label, None = default\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# Generator와 Discriminator의 학습률(Learning rate) 설정\n",
    "lr = 0.0025/8.\n",
    "\n",
    "# Beta1과 Beta2는 일반적으로 0.0과 0.99로 설정됩니다.\n",
    "beta1 = 0.0\n",
    "beta2 = 0.99\n",
    "\n",
    "# Generator Optimizer\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=2e-5, betas=(beta1, beta2))\n",
    "\n",
    "# Discriminator Optimizer\n",
    "d_optimizer = optim.Adam(discriminator.parameters(),\n",
    "                         lr=2e-4, betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_discriminator_loss(discriminator, generator, real_images, labels, z, device, r1_gamma, blur_init_sigma, blur_fade_kimg, augment_pipe, cur_nimg):\n",
    "    # 진짜와 가짜 이미지에 대한 예측\n",
    "    real_pred = discriminator(real_images, labels)\n",
    "    fake_images = generator(z, labels)\n",
    "    fake_pred = discriminator(fake_images.detach(), labels)  # labels 추가\n",
    "\n",
    "    # 손실 계산\n",
    "    loss_real = torch.nn.functional.softplus(-real_pred)\n",
    "    loss_fake = torch.nn.functional.softplus(fake_pred)\n",
    "    d_loss_val = loss_real + loss_fake\n",
    "\n",
    "    # R1 regularization\n",
    "    if r1_gamma > 0:\n",
    "        real_images.requires_grad = True\n",
    "        real_pred = discriminator(real_images, labels)  # labels 추가\n",
    "        r1_grads = torch.autograd.grad(outputs=real_pred.sum(\n",
    "        ), inputs=real_images, create_graph=True, allow_unused=False)[0]\n",
    "        r1_penalty = r1_grads.square().sum([1, 2, 3]).mean()\n",
    "        r1_loss = r1_penalty * (r1_gamma / 2)\n",
    "        d_loss_val += r1_loss\n",
    "\n",
    "    return d_loss_val.mean()\n",
    "\n",
    "\n",
    "def train_generator_loss(generator, discriminator, z, labels, pl_weight, pl_mean, pl_decay, pl_no_weight_grad):\n",
    "    # 가짜 이미지에 대한 예측\n",
    "    fake_images = generator(z, labels)\n",
    "    fake_pred = discriminator(fake_images, labels)  # labels 추가\n",
    "\n",
    "    # 손실 계산\n",
    "    g_loss_val = torch.nn.functional.softplus(-fake_pred)\n",
    "\n",
    "    # Path length regularization\n",
    "    if pl_weight > 0:\n",
    "        pl_noise = torch.randn_like(\n",
    "            fake_images) / np.sqrt(fake_images.shape[2] * fake_images.shape[3])\n",
    "        pl_grads = torch.autograd.grad(outputs=(\n",
    "            fake_images * pl_noise).sum(), inputs=z, create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "        # pl_grads의 크기를 기반으로 차원을 조정합니다.\n",
    "        # (1, 1024) -> sum over dim 1 and then sqrt\n",
    "        pl_lengths = pl_grads.square().sum(dim=1).sqrt()\n",
    "        pl_mean = pl_mean.lerp(pl_lengths.mean(), pl_decay)\n",
    "        pl_penalty = (pl_lengths - pl_mean).square()\n",
    "        g_loss_val += pl_penalty * pl_weight\n",
    "\n",
    "    return g_loss_val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 156/894 [00:29<02:19,  5.28it/s, epoch=1, gloss=31.4, dloss=0.116, batch per device=1, img shape=torch.Size([3, 256, 256])]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# -----------------\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#  Train Generator\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# -----------------\u001b[39;00m\n\u001b[1;32m     43\u001b[0m g_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 44\u001b[0m g_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_generator_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiscriminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscriminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpl_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpl_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Path Length 정규화 강도 적용\u001b[39;49;00m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpl_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpl_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpl_no_weight_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     53\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m g_loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# retain_graph를 사용하지 않고 역전파\u001b[39;00m\n\u001b[1;32m     56\u001b[0m g_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mtrain_generator_loss\u001b[0;34m(generator, discriminator, z, labels, pl_weight, pl_mean, pl_decay, pl_no_weight_grad)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pl_weight \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     35\u001b[0m     pl_noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(\n\u001b[1;32m     36\u001b[0m         fake_images) \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(fake_images\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m fake_images\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m3\u001b[39m])\n\u001b[0;32m---> 37\u001b[0m     pl_grads \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_images\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpl_noise\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# pl_grads의 크기를 기반으로 차원을 조정합니다.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# (1, 1024) -> sum over dim 1 and then sqrt\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     pl_lengths \u001b[38;5;241m=\u001b[39m pl_grads\u001b[38;5;241m.\u001b[39msquare()\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqrt()\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.10/site-packages/torch/autograd/__init__.py:412\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    408\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    409\u001b[0m         grad_outputs_\n\u001b[1;32m    410\u001b[0m     )\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 412\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    423\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    425\u001b[0m     ):\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "r1_gamma = 10  # R1 정규화 강도를 더 낮춤\n",
    "pl_weight = 1.0  # Path Length 정규화 강도를 크게 낮춤\n",
    "\n",
    "for epc in range(params['epochs']):\n",
    "    gloss_total = 0\n",
    "    dloss_total = 0\n",
    "    step = 0\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    with tqdm(dataloader, dynamic_ncols=True) as tqdmDataLoader:\n",
    "        for img, lab in tqdmDataLoader:\n",
    "            real_images = img.to(device)\n",
    "            labels = lab.to(device)\n",
    "            z = torch.randn(params['batch_size'], 256,\n",
    "                            device=device, requires_grad=True)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            d_optimizer.zero_grad()\n",
    "\n",
    "            # 손실 계산\n",
    "            d_loss = train_discriminator_loss(\n",
    "                discriminator=discriminator,\n",
    "                generator=generator,\n",
    "                real_images=real_images,\n",
    "                labels=labels,\n",
    "                z=z,\n",
    "                device=device,\n",
    "                r1_gamma=r1_gamma,  # R1 정규화 강도 적용\n",
    "                blur_init_sigma=0,\n",
    "                blur_fade_kimg=1000,\n",
    "                augment_pipe=None,\n",
    "                cur_nimg=step\n",
    "            )\n",
    "            d_loss.backward()\n",
    "            d_optimizer.step()\n",
    "            dloss_total += d_loss.item()\n",
    "\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "            g_optimizer.zero_grad()\n",
    "            g_loss = train_generator_loss(\n",
    "                generator=generator,\n",
    "                discriminator=discriminator,\n",
    "                z=z,\n",
    "                labels=labels,\n",
    "                pl_weight=pl_weight,  # Path Length 정규화 강도 적용\n",
    "                pl_mean=torch.zeros([]).to(device),\n",
    "                pl_decay=0.01,\n",
    "                pl_no_weight_grad=False\n",
    "            )\n",
    "\n",
    "            g_loss.backward()  # retain_graph를 사용하지 않고 역전파\n",
    "            g_optimizer.step()\n",
    "            gloss_total += g_loss.item()\n",
    "\n",
    "            step += 1\n",
    "\n",
    "            tqdmDataLoader.set_postfix(\n",
    "                ordered_dict={\n",
    "                    \"epoch\": epc + 1,\n",
    "                    \"gloss\": gloss_total / step,\n",
    "                    \"dloss\": dloss_total / step,\n",
    "                    \"batch per device\": real_images.shape[0],\n",
    "                    \"img shape\": real_images.shape[1:],\n",
    "                }\n",
    "            )\n",
    " # 이미지 생성 및 저장\n",
    "    generator.eval()  # 평가 모드로 전환\n",
    "    with torch.no_grad():\n",
    "        for cls_idx, cls_name in enumerate(class_list):\n",
    "            z = torch.randn(1, 1024, device=device)\n",
    "            labels = torch.zeros((1, len(class_list)), device=device)\n",
    "            labels[0, cls_idx] = 1\n",
    "            generated_images = generator(z, labels)\n",
    "            save_image(transback(generated_images),\n",
    "                       f'../../result/styleGan2/{cls_name}/generated_images_epoch_{epc+1}.png', nrow=4)\n",
    "\n",
    "    if epc % 10 == 0:\n",
    "        checkpoint = {\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'g_optimizer_state_dict': g_optimizer.state_dict(),\n",
    "            'd_optimizer_state_dict': d_optimizer.state_dict(),\n",
    "            'epoch': epc + 1\n",
    "        }\n",
    "        torch.save(\n",
    "            checkpoint, f'../../model/styleGan2/DWI/checkpoint_epoch_{epc+1}.pt')\n",
    "\n",
    "        # 학습 모드로 다시 전환\n",
    "        generator.train()\n",
    "\n",
    "        torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
