{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image\n",
    "import itertools\n",
    "import random\n",
    "import nibabel as nib\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "topilimage =transforms.ToPILImage()\n",
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model params\n",
    "params = {\n",
    "    'batch_size':64,              # 128x128이면 그대로 사용 가능 (GPU 메모리에 따라 128로 늘려도 OK)\n",
    "    'input_size':128,\n",
    "    'resize_scale':128,          # resize도 128로 고정\n",
    "    'crop_size':128,             # crop도 동일하게\n",
    "    'fliplr':False,              # 필요에 따라 True로 변경 가능 (augmentation 목적)\n",
    "    'num_epochs':500,            # 이미지 작아졌으니 500 정도로 줄여도 무방\n",
    "    'decay_epoch':25,            # 절반 시점에 decay\n",
    "\n",
    "    'ngf':16,                    # generator filter 수 절반으로 축소\n",
    "    'ndf':32,                    # discriminator filter 수도 축소\n",
    "    'num_resnet':3,              # resnet block 수도 줄이기 (128에서는 3~4 추천)\n",
    "    \n",
    "    'lrG':2e-4,                  # 이미지가 작아졌기 때문에 learning rate는 살짝 키워도 안정적 (2e-4)\n",
    "    'lrD':2e-4,                  #\n",
    "    'beta1':0.5,\n",
    "    'beta2':0.999,\n",
    "    \n",
    "    'lambdaA':10,\n",
    "    'lambdaB':10,\n",
    "    'img_form':'nii.gz'\n",
    "}\n",
    "\n",
    "data_dir = '../../data/registration_data/'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [01:32<00:00,  5.43it/s]\n"
     ]
    }
   ],
   "source": [
    "def to_np(x):\n",
    "    return x.data.cpu().numpy()\n",
    "class ImagePool():\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "        if self.pool_size > 0:\n",
    "            self.num_imgs = 0\n",
    "            self.images = []\n",
    "\n",
    "    def query(self, images):\n",
    "        if self.pool_size == 0:\n",
    "            return images\n",
    "        return_images = []\n",
    "        with torch.no_grad():  # No need to track gradients\n",
    "            for image in images:\n",
    "                image = torch.unsqueeze(image, 0)\n",
    "                if self.num_imgs < self.pool_size:\n",
    "                    self.num_imgs += 1\n",
    "                    self.images.append(image)\n",
    "                    return_images.append(image)\n",
    "                else:\n",
    "                    p = random.uniform(0, 1)\n",
    "                    if p > 0.5:\n",
    "                        random_id = random.randint(0, self.pool_size - 1)\n",
    "                        tmp = self.images[random_id].clone()\n",
    "                        self.images[random_id] = image\n",
    "                        return_images.append(tmp)\n",
    "                    else:\n",
    "                        return_images.append(image)\n",
    "            return_images = torch.cat(return_images, 0)\n",
    "        return return_images.detach()  \n",
    "        \n",
    "class DatasetFromFolder(data.Dataset):\n",
    "    def __init__(self, mri_image_list,ct_image_list):\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        self.mri_image_list =mri_image_list\n",
    "        self.ct_image_list =ct_image_list\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load Image\n",
    "        img = self.mri_image_list[index]\n",
    "        target = self.ct_image_list[index]\n",
    "        return img, target\n",
    "    def __len__(self):\n",
    "        return len(self.mri_image_list)\n",
    "    \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=params['input_size']),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "data_a_list=glob(data_dir+'registration_DWI/*.'+params['img_form'])\n",
    "data_b_list=[f.replace('/registration_DWI', '/CT') for f in data_a_list]\n",
    "MRI_img_tensor=torch.zeros(len(data_a_list)*40,1,params['input_size'],params['input_size'])\n",
    "CT_img_tensor=torch.zeros(len(data_b_list)*40,1,params['input_size'],params['input_size'])\n",
    "for i in tqdm(range(len(data_a_list))):\n",
    "    nib_img=nib.load(data_a_list[i])\n",
    "    mri_img=nib_img.get_fdata()\n",
    "    mri_img_tensor = torch.from_numpy(mri_img).unsqueeze(1).float()-1.  # (40, 1, 256, 256)\n",
    "\n",
    "    # Resize to (40, 1, 128, 128)\n",
    "    img_tensor_resized = F.interpolate(mri_img_tensor, size=(128, 128), mode='bilinear', align_corners=False)\n",
    "\n",
    "    MRI_img_tensor[i*40:(i+1)*40] = img_tensor_resized\n",
    "\n",
    "    ct_img=nib.load(data_b_list[i])\n",
    "    ct_img=ct_img.get_fdata()\n",
    "    ct_img_tensor = torch.from_numpy(ct_img).unsqueeze(1).float()-1.  # (40, 1, 256, 256)\n",
    "    # Resize to (40, 1, 128, 128)\n",
    "    img_tensor_resized = F.interpolate(ct_img_tensor, size=(128, 128), mode='bilinear', align_corners=False)\n",
    "    CT_img_tensor[i*40:(i+1)*40] = img_tensor_resized\n",
    "train_data_A = DatasetFromFolder(MRI_img_tensor,CT_img_tensor)\n",
    "loader = torch.utils.data.DataLoader(dataset=train_data_A , batch_size=params['batch_size'], shuffle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CycleGAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(features, features, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(features, features, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(features),\n",
    "            nn.Dropout(0.5)  # Dropout 추가 (드롭아웃 확률 0.5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "# Generator Model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, n_residual_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "        # 초기 컨볼루션 블록\n",
    "        model = [\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=7, padding=3),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "\n",
    "        # 다운샘플링\n",
    "        in_features = 64\n",
    "        out_features = in_features * 2\n",
    "        for _ in range(4):  # 기존 2에서 4로 변경\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "\n",
    "        # 잔차 블록\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "\n",
    "        # 업샘플링\n",
    "        out_features = in_features // 2\n",
    "        for _ in range(4):  # 기존 2에서 4로 변경\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features // 2\n",
    "\n",
    "        # 출력 레이어\n",
    "        model += [nn.Conv2d(64, output_channels, kernel_size=7, padding=3), nn.Tanh()]\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Discriminator Model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, kernel_size=4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(input_channels, 64, normalization=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(1, 1).to(device)  # 그레이스케일에서 컬러로\n",
    "F = Generator(1, 1).to(device)  # 컬러에서 그레이스케일로\n",
    "D_ct = Discriminator(1).to(device)\n",
    "D_mri = Discriminator(1).to(device)\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizer_G = optim.Adam(itertools.chain(G.parameters(), F.parameters()), lr=2e-5, betas=(0.5, 0.999))\n",
    "optimizer_D_ct = optim.Adam(D_ct.parameters(), lr=2e-5, betas=(0.5, 0.999))\n",
    "optimizer_D_mri = optim.Adam(D_mri.parameters(), lr=2e-5, betas=(0.5, 0.999))\n",
    "\n",
    "# 학습률 스케줄러 추가\n",
    "lr_scheduler_G = optim.lr_scheduler.StepLR(optimizer_G, step_size=20, gamma=0.5)\n",
    "lr_scheduler_D_ct = optim.lr_scheduler.StepLR(optimizer_D_ct, step_size=20, gamma=0.5)\n",
    "lr_scheduler_D_mri = optim.lr_scheduler.StepLR(optimizer_D_mri, step_size=20, gamma=0.5)\n",
    "\n",
    "# 손실 함수 설정\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=5.9997, Loss D_ct=0.1829, Loss D_mri=0.1848] \n",
      "Epoch 2/1000: 100%|██████████| 313/313 [04:11<00:00,  1.24it/s, Loss G=2.6032, Loss D_ct=0.1999, Loss D_mri=0.2006]\n",
      "Epoch 3/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=2.3145, Loss D_ct=0.2093, Loss D_mri=0.2059]\n",
      "Epoch 4/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=2.1667, Loss D_ct=0.2157, Loss D_mri=0.2101]\n",
      "Epoch 5/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=2.0473, Loss D_ct=0.2254, Loss D_mri=0.2156]\n",
      "Epoch 6/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.9571, Loss D_ct=0.2291, Loss D_mri=0.2206]\n",
      "Epoch 7/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.9033, Loss D_ct=0.2335, Loss D_mri=0.2235]\n",
      "Epoch 8/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.8514, Loss D_ct=0.2336, Loss D_mri=0.2269]\n",
      "Epoch 9/1000: 100%|██████████| 313/313 [04:11<00:00,  1.25it/s, Loss G=1.8154, Loss D_ct=0.2324, Loss D_mri=0.2282]\n",
      "Epoch 10/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.7839, Loss D_ct=0.2343, Loss D_mri=0.2298]\n",
      "Epoch 11/1000: 100%|██████████| 313/313 [04:11<00:00,  1.25it/s, Loss G=1.7554, Loss D_ct=0.2334, Loss D_mri=0.2314]\n",
      "Epoch 12/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.7582, Loss D_ct=0.2340, Loss D_mri=0.2313]\n",
      "Epoch 13/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.7090, Loss D_ct=0.2360, Loss D_mri=0.2331]\n",
      "Epoch 14/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.6806, Loss D_ct=0.2369, Loss D_mri=0.2353]\n",
      "Epoch 15/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.6622, Loss D_ct=0.2403, Loss D_mri=0.2356]\n",
      "Epoch 16/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.6505, Loss D_ct=0.2398, Loss D_mri=0.2347]\n",
      "Epoch 17/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.6542, Loss D_ct=0.2359, Loss D_mri=0.2333]\n",
      "Epoch 18/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.6468, Loss D_ct=0.2312, Loss D_mri=0.2345]\n",
      "Epoch 19/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.6341, Loss D_ct=0.2309, Loss D_mri=0.2362]\n",
      "Epoch 20/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.6247, Loss D_ct=0.2310, Loss D_mri=0.2375]\n",
      "Epoch 21/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.6144, Loss D_ct=0.2305, Loss D_mri=0.2379]\n",
      "Epoch 22/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.6060, Loss D_ct=0.2307, Loss D_mri=0.2377]\n",
      "Epoch 23/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.5913, Loss D_ct=0.2311, Loss D_mri=0.2391]\n",
      "Epoch 24/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.5829, Loss D_ct=0.2329, Loss D_mri=0.2391]\n",
      "Epoch 25/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.5842, Loss D_ct=0.2333, Loss D_mri=0.2373]\n",
      "Epoch 26/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.5574, Loss D_ct=0.2368, Loss D_mri=0.2392]\n",
      "Epoch 27/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.5470, Loss D_ct=0.2371, Loss D_mri=0.2396]\n",
      "Epoch 28/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.5490, Loss D_ct=0.2376, Loss D_mri=0.2363]\n",
      "Epoch 29/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.5350, Loss D_ct=0.2377, Loss D_mri=0.2386]\n",
      "Epoch 30/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.5333, Loss D_ct=0.2376, Loss D_mri=0.2385]\n",
      "Epoch 31/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.5163, Loss D_ct=0.2390, Loss D_mri=0.2400]\n",
      "Epoch 32/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.5126, Loss D_ct=0.2391, Loss D_mri=0.2401]\n",
      "Epoch 33/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.5107, Loss D_ct=0.2371, Loss D_mri=0.2393]\n",
      "Epoch 34/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.5040, Loss D_ct=0.2390, Loss D_mri=0.2387]\n",
      "Epoch 35/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.4915, Loss D_ct=0.2399, Loss D_mri=0.2394]\n",
      "Epoch 36/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.4928, Loss D_ct=0.2389, Loss D_mri=0.2399]\n",
      "Epoch 37/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.4867, Loss D_ct=0.2391, Loss D_mri=0.2397]\n",
      "Epoch 38/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.4827, Loss D_ct=0.2378, Loss D_mri=0.2407]\n",
      "Epoch 39/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.4786, Loss D_ct=0.2396, Loss D_mri=0.2407]\n",
      "Epoch 40/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.4850, Loss D_ct=0.2382, Loss D_mri=0.2377]\n",
      "Epoch 41/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.4627, Loss D_ct=0.2385, Loss D_mri=0.2411]\n",
      "Epoch 42/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.4581, Loss D_ct=0.2379, Loss D_mri=0.2422]\n",
      "Epoch 43/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.4576, Loss D_ct=0.2378, Loss D_mri=0.2410]\n",
      "Epoch 44/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.4527, Loss D_ct=0.2376, Loss D_mri=0.2416]\n",
      "Epoch 45/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.4567, Loss D_ct=0.2373, Loss D_mri=0.2396]\n",
      "Epoch 46/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.4443, Loss D_ct=0.2379, Loss D_mri=0.2416]\n",
      "Epoch 47/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.4563, Loss D_ct=0.2376, Loss D_mri=0.2405]\n",
      "Epoch 48/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.4358, Loss D_ct=0.2357, Loss D_mri=0.2406]\n",
      "Epoch 49/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.4373, Loss D_ct=0.2360, Loss D_mri=0.2422]\n",
      "Epoch 50/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.4299, Loss D_ct=0.2370, Loss D_mri=0.2419]\n",
      "Epoch 51/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.4203, Loss D_ct=0.2379, Loss D_mri=0.2429]\n",
      "Epoch 52/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.4186, Loss D_ct=0.2378, Loss D_mri=0.2431]\n",
      "Epoch 53/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.4155, Loss D_ct=0.2369, Loss D_mri=0.2428]\n",
      "Epoch 54/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.4262, Loss D_ct=0.2357, Loss D_mri=0.2423]\n",
      "Epoch 55/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.4064, Loss D_ct=0.2369, Loss D_mri=0.2435]\n",
      "Epoch 56/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.4067, Loss D_ct=0.2353, Loss D_mri=0.2436]\n",
      "Epoch 57/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.4012, Loss D_ct=0.2357, Loss D_mri=0.2434]\n",
      "Epoch 58/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.3989, Loss D_ct=0.2370, Loss D_mri=0.2427]\n",
      "Epoch 59/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.4002, Loss D_ct=0.2361, Loss D_mri=0.2427]\n",
      "Epoch 60/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.3924, Loss D_ct=0.2377, Loss D_mri=0.2436]\n",
      "Epoch 61/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.3854, Loss D_ct=0.2372, Loss D_mri=0.2431]\n",
      "Epoch 62/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.3820, Loss D_ct=0.2364, Loss D_mri=0.2431]\n",
      "Epoch 63/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.3845, Loss D_ct=0.2365, Loss D_mri=0.2430]\n",
      "Epoch 64/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.3846, Loss D_ct=0.2372, Loss D_mri=0.2427]\n",
      "Epoch 65/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.3828, Loss D_ct=0.2334, Loss D_mri=0.2428]\n",
      "Epoch 66/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.3771, Loss D_ct=0.2361, Loss D_mri=0.2421]\n",
      "Epoch 67/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.3725, Loss D_ct=0.2372, Loss D_mri=0.2431]\n",
      "Epoch 68/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.3703, Loss D_ct=0.2363, Loss D_mri=0.2423]\n",
      "Epoch 69/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.3722, Loss D_ct=0.2340, Loss D_mri=0.2421]\n",
      "Epoch 70/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.3702, Loss D_ct=0.2339, Loss D_mri=0.2427]\n",
      "Epoch 71/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.3611, Loss D_ct=0.2360, Loss D_mri=0.2423]\n",
      "Epoch 72/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.3569, Loss D_ct=0.2363, Loss D_mri=0.2423]\n",
      "Epoch 73/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.3569, Loss D_ct=0.2357, Loss D_mri=0.2428]\n",
      "Epoch 74/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.3576, Loss D_ct=0.2356, Loss D_mri=0.2430]\n",
      "Epoch 75/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.3466, Loss D_ct=0.2334, Loss D_mri=0.2430]\n",
      "Epoch 76/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.3526, Loss D_ct=0.2365, Loss D_mri=0.2416]\n",
      "Epoch 77/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.3441, Loss D_ct=0.2379, Loss D_mri=0.2413]\n",
      "Epoch 78/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.3369, Loss D_ct=0.2362, Loss D_mri=0.2422]\n",
      "Epoch 79/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.3347, Loss D_ct=0.2364, Loss D_mri=0.2430]\n",
      "Epoch 80/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.3387, Loss D_ct=0.2364, Loss D_mri=0.2419]\n",
      "Epoch 81/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.3326, Loss D_ct=0.2355, Loss D_mri=0.2417]\n",
      "Epoch 82/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.3396, Loss D_ct=0.2355, Loss D_mri=0.2410]\n",
      "Epoch 83/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.3320, Loss D_ct=0.2333, Loss D_mri=0.2422]\n",
      "Epoch 84/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.3255, Loss D_ct=0.2343, Loss D_mri=0.2424]\n",
      "Epoch 85/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.3249, Loss D_ct=0.2349, Loss D_mri=0.2421]\n",
      "Epoch 86/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.3167, Loss D_ct=0.2346, Loss D_mri=0.2426]\n",
      "Epoch 87/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.3264, Loss D_ct=0.2336, Loss D_mri=0.2415]\n",
      "Epoch 88/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.3174, Loss D_ct=0.2332, Loss D_mri=0.2413]\n",
      "Epoch 89/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.3124, Loss D_ct=0.2326, Loss D_mri=0.2431]\n",
      "Epoch 90/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.3139, Loss D_ct=0.2328, Loss D_mri=0.2418]\n",
      "Epoch 91/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.3138, Loss D_ct=0.2315, Loss D_mri=0.2418]\n",
      "Epoch 92/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.3051, Loss D_ct=0.2329, Loss D_mri=0.2419]\n",
      "Epoch 93/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.3034, Loss D_ct=0.2332, Loss D_mri=0.2422]\n",
      "Epoch 94/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.3025, Loss D_ct=0.2322, Loss D_mri=0.2414]\n",
      "Epoch 95/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.3081, Loss D_ct=0.2318, Loss D_mri=0.2395]\n",
      "Epoch 96/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.3022, Loss D_ct=0.2305, Loss D_mri=0.2414]\n",
      "Epoch 97/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2996, Loss D_ct=0.2304, Loss D_mri=0.2424]\n",
      "Epoch 98/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2972, Loss D_ct=0.2289, Loss D_mri=0.2417]\n",
      "Epoch 99/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2980, Loss D_ct=0.2302, Loss D_mri=0.2414]\n",
      "Epoch 100/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2947, Loss D_ct=0.2297, Loss D_mri=0.2422]\n",
      "Epoch 101/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2932, Loss D_ct=0.2270, Loss D_mri=0.2425]\n",
      "Epoch 102/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2968, Loss D_ct=0.2271, Loss D_mri=0.2421]\n",
      "Epoch 103/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2951, Loss D_ct=0.2271, Loss D_mri=0.2412]\n",
      "Epoch 104/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2961, Loss D_ct=0.2252, Loss D_mri=0.2424]\n",
      "Epoch 105/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2915, Loss D_ct=0.2267, Loss D_mri=0.2425]\n",
      "Epoch 106/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2887, Loss D_ct=0.2270, Loss D_mri=0.2416]\n",
      "Epoch 107/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2940, Loss D_ct=0.2269, Loss D_mri=0.2398]\n",
      "Epoch 108/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2890, Loss D_ct=0.2257, Loss D_mri=0.2409]\n",
      "Epoch 109/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2870, Loss D_ct=0.2256, Loss D_mri=0.2418]\n",
      "Epoch 110/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2837, Loss D_ct=0.2272, Loss D_mri=0.2424]\n",
      "Epoch 111/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2814, Loss D_ct=0.2267, Loss D_mri=0.2416]\n",
      "Epoch 112/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2826, Loss D_ct=0.2251, Loss D_mri=0.2420]\n",
      "Epoch 113/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2819, Loss D_ct=0.2255, Loss D_mri=0.2418]\n",
      "Epoch 114/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2829, Loss D_ct=0.2223, Loss D_mri=0.2419]\n",
      "Epoch 115/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2817, Loss D_ct=0.2250, Loss D_mri=0.2414]\n",
      "Epoch 116/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2840, Loss D_ct=0.2254, Loss D_mri=0.2407]\n",
      "Epoch 117/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2786, Loss D_ct=0.2241, Loss D_mri=0.2412]\n",
      "Epoch 118/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2786, Loss D_ct=0.2225, Loss D_mri=0.2416]\n",
      "Epoch 119/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2989, Loss D_ct=0.2216, Loss D_mri=0.2398]\n",
      "Epoch 120/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2772, Loss D_ct=0.2229, Loss D_mri=0.2418]\n",
      "Epoch 121/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2787, Loss D_ct=0.2207, Loss D_mri=0.2408]\n",
      "Epoch 122/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2845, Loss D_ct=0.2186, Loss D_mri=0.2415]\n",
      "Epoch 123/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2794, Loss D_ct=0.2197, Loss D_mri=0.2420]\n",
      "Epoch 124/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2730, Loss D_ct=0.2214, Loss D_mri=0.2413]\n",
      "Epoch 125/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2727, Loss D_ct=0.2220, Loss D_mri=0.2418]\n",
      "Epoch 126/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2755, Loss D_ct=0.2197, Loss D_mri=0.2407]\n",
      "Epoch 127/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2765, Loss D_ct=0.2188, Loss D_mri=0.2417]\n",
      "Epoch 128/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2728, Loss D_ct=0.2195, Loss D_mri=0.2417]\n",
      "Epoch 129/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2791, Loss D_ct=0.2186, Loss D_mri=0.2409]\n",
      "Epoch 130/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2690, Loss D_ct=0.2187, Loss D_mri=0.2420]\n",
      "Epoch 131/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2637, Loss D_ct=0.2243, Loss D_mri=0.2415]\n",
      "Epoch 132/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2697, Loss D_ct=0.2215, Loss D_mri=0.2412]\n",
      "Epoch 133/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2685, Loss D_ct=0.2195, Loss D_mri=0.2415]\n",
      "Epoch 134/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2711, Loss D_ct=0.2182, Loss D_mri=0.2408]\n",
      "Epoch 135/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2752, Loss D_ct=0.2188, Loss D_mri=0.2382]\n",
      "Epoch 136/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2903, Loss D_ct=0.2156, Loss D_mri=0.2347]\n",
      "Epoch 137/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2713, Loss D_ct=0.2169, Loss D_mri=0.2415]\n",
      "Epoch 138/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2713, Loss D_ct=0.2156, Loss D_mri=0.2413]\n",
      "Epoch 139/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2703, Loss D_ct=0.2167, Loss D_mri=0.2422]\n",
      "Epoch 140/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2669, Loss D_ct=0.2201, Loss D_mri=0.2418]\n",
      "Epoch 141/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2690, Loss D_ct=0.2157, Loss D_mri=0.2419]\n",
      "Epoch 142/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2662, Loss D_ct=0.2164, Loss D_mri=0.2412]\n",
      "Epoch 143/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2684, Loss D_ct=0.2161, Loss D_mri=0.2409]\n",
      "Epoch 144/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2730, Loss D_ct=0.2140, Loss D_mri=0.2402]\n",
      "Epoch 145/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2788, Loss D_ct=0.2116, Loss D_mri=0.2411]\n",
      "Epoch 146/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2706, Loss D_ct=0.2143, Loss D_mri=0.2411]\n",
      "Epoch 147/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2725, Loss D_ct=0.2136, Loss D_mri=0.2400]\n",
      "Epoch 148/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2722, Loss D_ct=0.2111, Loss D_mri=0.2420]\n",
      "Epoch 149/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2727, Loss D_ct=0.2129, Loss D_mri=0.2391]\n",
      "Epoch 150/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2735, Loss D_ct=0.2092, Loss D_mri=0.2417]\n",
      "Epoch 151/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2643, Loss D_ct=0.2160, Loss D_mri=0.2420]\n",
      "Epoch 152/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2680, Loss D_ct=0.2122, Loss D_mri=0.2409]\n",
      "Epoch 153/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2831, Loss D_ct=0.2104, Loss D_mri=0.2389]\n",
      "Epoch 154/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2842, Loss D_ct=0.2094, Loss D_mri=0.2362]\n",
      "Epoch 155/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2736, Loss D_ct=0.2096, Loss D_mri=0.2401]\n",
      "Epoch 156/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2654, Loss D_ct=0.2110, Loss D_mri=0.2423]\n",
      "Epoch 157/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2648, Loss D_ct=0.2192, Loss D_mri=0.2408]\n",
      "Epoch 158/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2828, Loss D_ct=0.2189, Loss D_mri=0.2392]\n",
      "Epoch 159/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2895, Loss D_ct=0.2129, Loss D_mri=0.2335]\n",
      "Epoch 160/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2824, Loss D_ct=0.2115, Loss D_mri=0.2363]\n",
      "Epoch 161/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2764, Loss D_ct=0.2078, Loss D_mri=0.2389]\n",
      "Epoch 162/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2739, Loss D_ct=0.2088, Loss D_mri=0.2406]\n",
      "Epoch 163/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2681, Loss D_ct=0.2094, Loss D_mri=0.2409]\n",
      "Epoch 164/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2817, Loss D_ct=0.2082, Loss D_mri=0.2363]\n",
      "Epoch 165/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2823, Loss D_ct=0.2091, Loss D_mri=0.2359]\n",
      "Epoch 166/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2736, Loss D_ct=0.2071, Loss D_mri=0.2395]\n",
      "Epoch 167/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2744, Loss D_ct=0.2073, Loss D_mri=0.2399]\n",
      "Epoch 168/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2705, Loss D_ct=0.2071, Loss D_mri=0.2407]\n",
      "Epoch 169/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2780, Loss D_ct=0.2053, Loss D_mri=0.2406]\n",
      "Epoch 170/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2721, Loss D_ct=0.2050, Loss D_mri=0.2406]\n",
      "Epoch 171/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2681, Loss D_ct=0.2110, Loss D_mri=0.2409]\n",
      "Epoch 172/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2709, Loss D_ct=0.2055, Loss D_mri=0.2408]\n",
      "Epoch 173/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2763, Loss D_ct=0.2069, Loss D_mri=0.2395]\n",
      "Epoch 174/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2748, Loss D_ct=0.2061, Loss D_mri=0.2408]\n",
      "Epoch 175/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2701, Loss D_ct=0.2050, Loss D_mri=0.2411]\n",
      "Epoch 176/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2655, Loss D_ct=0.2072, Loss D_mri=0.2414]\n",
      "Epoch 177/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2707, Loss D_ct=0.2044, Loss D_mri=0.2420]\n",
      "Epoch 178/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2764, Loss D_ct=0.2035, Loss D_mri=0.2403]\n",
      "Epoch 179/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2758, Loss D_ct=0.2034, Loss D_mri=0.2409]\n",
      "Epoch 180/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2813, Loss D_ct=0.2038, Loss D_mri=0.2395]\n",
      "Epoch 181/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2754, Loss D_ct=0.2028, Loss D_mri=0.2392]\n",
      "Epoch 182/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2714, Loss D_ct=0.2024, Loss D_mri=0.2404]\n",
      "Epoch 183/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2757, Loss D_ct=0.2036, Loss D_mri=0.2405]\n",
      "Epoch 184/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2766, Loss D_ct=0.2050, Loss D_mri=0.2395]\n",
      "Epoch 185/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2778, Loss D_ct=0.2061, Loss D_mri=0.2369]\n",
      "Epoch 186/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2806, Loss D_ct=0.2035, Loss D_mri=0.2357]\n",
      "Epoch 187/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2771, Loss D_ct=0.2043, Loss D_mri=0.2378]\n",
      "Epoch 188/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2859, Loss D_ct=0.2025, Loss D_mri=0.2387]\n",
      "Epoch 189/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2783, Loss D_ct=0.1992, Loss D_mri=0.2398]\n",
      "Epoch 190/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2784, Loss D_ct=0.2004, Loss D_mri=0.2394]\n",
      "Epoch 191/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2806, Loss D_ct=0.2010, Loss D_mri=0.2395]\n",
      "Epoch 192/1000: 100%|██████████| 313/313 [04:11<00:00,  1.25it/s, Loss G=1.2749, Loss D_ct=0.1991, Loss D_mri=0.2408]\n",
      "Epoch 193/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2855, Loss D_ct=0.1983, Loss D_mri=0.2386]\n",
      "Epoch 194/1000: 100%|██████████| 313/313 [04:11<00:00,  1.25it/s, Loss G=1.2770, Loss D_ct=0.2088, Loss D_mri=0.2381]\n",
      "Epoch 195/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2700, Loss D_ct=0.2051, Loss D_mri=0.2403]\n",
      "Epoch 196/1000: 100%|██████████| 313/313 [04:11<00:00,  1.25it/s, Loss G=1.2803, Loss D_ct=0.2011, Loss D_mri=0.2365]\n",
      "Epoch 197/1000: 100%|██████████| 313/313 [04:11<00:00,  1.25it/s, Loss G=1.2759, Loss D_ct=0.2025, Loss D_mri=0.2382]\n",
      "Epoch 198/1000: 100%|██████████| 313/313 [04:11<00:00,  1.25it/s, Loss G=1.2728, Loss D_ct=0.2020, Loss D_mri=0.2397]\n",
      "Epoch 199/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2724, Loss D_ct=0.2025, Loss D_mri=0.2395]\n",
      "Epoch 200/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2813, Loss D_ct=0.2008, Loss D_mri=0.2396]\n",
      "Epoch 201/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2800, Loss D_ct=0.1998, Loss D_mri=0.2396]\n",
      "Epoch 202/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2797, Loss D_ct=0.1991, Loss D_mri=0.2393]\n",
      "Epoch 203/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2797, Loss D_ct=0.1989, Loss D_mri=0.2389]\n",
      "Epoch 204/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2806, Loss D_ct=0.1995, Loss D_mri=0.2381]\n",
      "Epoch 205/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2751, Loss D_ct=0.2000, Loss D_mri=0.2409]\n",
      "Epoch 206/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2741, Loss D_ct=0.2064, Loss D_mri=0.2396]\n",
      "Epoch 207/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.3059, Loss D_ct=0.2110, Loss D_mri=0.2294]\n",
      "Epoch 208/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.3047, Loss D_ct=0.2019, Loss D_mri=0.2312]\n",
      "Epoch 209/1000: 100%|██████████| 313/313 [04:11<00:00,  1.24it/s, Loss G=1.2839, Loss D_ct=0.2001, Loss D_mri=0.2370]\n",
      "Epoch 210/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2806, Loss D_ct=0.1977, Loss D_mri=0.2402]\n",
      "Epoch 211/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2734, Loss D_ct=0.1984, Loss D_mri=0.2414]\n",
      "Epoch 212/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2755, Loss D_ct=0.2003, Loss D_mri=0.2393]\n",
      "Epoch 213/1000: 100%|██████████| 313/313 [04:09<00:00,  1.25it/s, Loss G=1.2789, Loss D_ct=0.1983, Loss D_mri=0.2398]\n",
      "Epoch 214/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2748, Loss D_ct=0.1980, Loss D_mri=0.2407]\n",
      "Epoch 215/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2776, Loss D_ct=0.2020, Loss D_mri=0.2395]\n",
      "Epoch 216/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2777, Loss D_ct=0.2012, Loss D_mri=0.2375]\n",
      "Epoch 217/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2832, Loss D_ct=0.1971, Loss D_mri=0.2389]\n",
      "Epoch 218/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2775, Loss D_ct=0.1966, Loss D_mri=0.2395]\n",
      "Epoch 219/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2775, Loss D_ct=0.1974, Loss D_mri=0.2415]\n",
      "Epoch 220/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2808, Loss D_ct=0.1976, Loss D_mri=0.2399]\n",
      "Epoch 221/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=2.0592, Loss D_ct=0.4710, Loss D_mri=0.2593]\n",
      "Epoch 222/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.3492, Loss D_ct=0.2691, Loss D_mri=0.2301]\n",
      "Epoch 223/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2567, Loss D_ct=0.2572, Loss D_mri=0.2260]\n",
      "Epoch 224/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2180, Loss D_ct=0.2524, Loss D_mri=0.2288]\n",
      "Epoch 225/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2239, Loss D_ct=0.2339, Loss D_mri=0.2321]\n",
      "Epoch 226/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2463, Loss D_ct=0.2126, Loss D_mri=0.2342]\n",
      "Epoch 227/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2668, Loss D_ct=0.1991, Loss D_mri=0.2345]\n",
      "Epoch 228/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2684, Loss D_ct=0.1933, Loss D_mri=0.2379]\n",
      "Epoch 229/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2649, Loss D_ct=0.1948, Loss D_mri=0.2402]\n",
      "Epoch 230/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2602, Loss D_ct=0.1969, Loss D_mri=0.2399]\n",
      "Epoch 231/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2658, Loss D_ct=0.1953, Loss D_mri=0.2390]\n",
      "Epoch 232/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2672, Loss D_ct=0.1995, Loss D_mri=0.2363]\n",
      "Epoch 233/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2661, Loss D_ct=0.1970, Loss D_mri=0.2406]\n",
      "Epoch 234/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2736, Loss D_ct=0.1964, Loss D_mri=0.2405]\n",
      "Epoch 235/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2697, Loss D_ct=0.1981, Loss D_mri=0.2410]\n",
      "Epoch 236/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2750, Loss D_ct=0.1996, Loss D_mri=0.2368]\n",
      "Epoch 237/1000: 100%|██████████| 313/313 [04:11<00:00,  1.24it/s, Loss G=1.2690, Loss D_ct=0.2005, Loss D_mri=0.2384]\n",
      "Epoch 238/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2708, Loss D_ct=0.1989, Loss D_mri=0.2388]\n",
      "Epoch 239/1000: 100%|██████████| 313/313 [04:11<00:00,  1.25it/s, Loss G=1.2707, Loss D_ct=0.2010, Loss D_mri=0.2390]\n",
      "Epoch 240/1000: 100%|██████████| 313/313 [04:11<00:00,  1.25it/s, Loss G=1.2775, Loss D_ct=0.2063, Loss D_mri=0.2358]\n",
      "Epoch 241/1000: 100%|██████████| 313/313 [04:11<00:00,  1.25it/s, Loss G=1.2779, Loss D_ct=0.1993, Loss D_mri=0.2358]\n",
      "Epoch 242/1000: 100%|██████████| 313/313 [04:11<00:00,  1.25it/s, Loss G=1.2873, Loss D_ct=0.1960, Loss D_mri=0.2355]\n",
      "Epoch 243/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2838, Loss D_ct=0.1987, Loss D_mri=0.2353]\n",
      "Epoch 244/1000: 100%|██████████| 313/313 [04:11<00:00,  1.25it/s, Loss G=1.2814, Loss D_ct=0.1985, Loss D_mri=0.2357]\n",
      "Epoch 245/1000: 100%|██████████| 313/313 [04:11<00:00,  1.25it/s, Loss G=1.2798, Loss D_ct=0.1984, Loss D_mri=0.2357]\n",
      "Epoch 246/1000: 100%|██████████| 313/313 [04:11<00:00,  1.25it/s, Loss G=1.2730, Loss D_ct=0.1999, Loss D_mri=0.2362]\n",
      "Epoch 247/1000: 100%|██████████| 313/313 [04:11<00:00,  1.24it/s, Loss G=1.2701, Loss D_ct=0.1983, Loss D_mri=0.2403]\n",
      "Epoch 248/1000: 100%|██████████| 313/313 [04:11<00:00,  1.25it/s, Loss G=1.2682, Loss D_ct=0.1968, Loss D_mri=0.2416]\n",
      "Epoch 249/1000: 100%|██████████| 313/313 [04:11<00:00,  1.24it/s, Loss G=1.2732, Loss D_ct=0.1984, Loss D_mri=0.2399]\n",
      "Epoch 250/1000: 100%|██████████| 313/313 [04:11<00:00,  1.24it/s, Loss G=1.2734, Loss D_ct=0.1949, Loss D_mri=0.2399]\n",
      "Epoch 251/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2752, Loss D_ct=0.2021, Loss D_mri=0.2391]\n",
      "Epoch 252/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2698, Loss D_ct=0.1988, Loss D_mri=0.2376]\n",
      "Epoch 253/1000: 100%|██████████| 313/313 [04:11<00:00,  1.24it/s, Loss G=1.2693, Loss D_ct=0.2013, Loss D_mri=0.2384]\n",
      "Epoch 254/1000: 100%|██████████| 313/313 [04:11<00:00,  1.24it/s, Loss G=1.2782, Loss D_ct=0.1972, Loss D_mri=0.2359]\n",
      "Epoch 255/1000: 100%|██████████| 313/313 [04:11<00:00,  1.24it/s, Loss G=1.2819, Loss D_ct=0.1965, Loss D_mri=0.2353]\n",
      "Epoch 256/1000: 100%|██████████| 313/313 [04:11<00:00,  1.25it/s, Loss G=1.2747, Loss D_ct=0.1998, Loss D_mri=0.2350]\n",
      "Epoch 257/1000: 100%|██████████| 313/313 [04:11<00:00,  1.25it/s, Loss G=1.2803, Loss D_ct=0.1972, Loss D_mri=0.2352]\n",
      "Epoch 258/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2823, Loss D_ct=0.2003, Loss D_mri=0.2360]\n",
      "Epoch 259/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2897, Loss D_ct=0.1940, Loss D_mri=0.2348]\n",
      "Epoch 260/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2825, Loss D_ct=0.1959, Loss D_mri=0.2355]\n",
      "Epoch 261/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2854, Loss D_ct=0.2019, Loss D_mri=0.2345]\n",
      "Epoch 262/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2811, Loss D_ct=0.1957, Loss D_mri=0.2346]\n",
      "Epoch 263/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2745, Loss D_ct=0.1959, Loss D_mri=0.2398]\n",
      "Epoch 264/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2736, Loss D_ct=0.1950, Loss D_mri=0.2399]\n",
      "Epoch 265/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.2669, Loss D_ct=0.2022, Loss D_mri=0.2387]\n",
      "Epoch 266/1000: 100%|██████████| 313/313 [04:10<00:00,  1.25it/s, Loss G=1.3134, Loss D_ct=0.2117, Loss D_mri=0.2321]\n",
      "Epoch 267/1000:  49%|████▊     | 152/313 [02:02<02:09,  1.24it/s, Loss G=1.2955, Loss D_ct=0.1990, Loss D_mri=0.2294]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# 총 생성자 손실\u001b[39;00m\n\u001b[1;32m     36\u001b[0m loss_G \u001b[38;5;241m=\u001b[39m loss_id_G \u001b[38;5;241m+\u001b[39m loss_id_F \u001b[38;5;241m+\u001b[39m loss_GAN_G \u001b[38;5;241m+\u001b[39m loss_GAN_F \u001b[38;5;241m+\u001b[39m loss_cycle_GF \u001b[38;5;241m+\u001b[39m loss_cycle_FG\n\u001b[0;32m---> 37\u001b[0m \u001b[43mloss_G\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m optimizer_G\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# 판별자 D_ct 업데이트\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(params['num_epochs']):\n",
    "    \n",
    "    total_loss_G=0\n",
    "    total_loss_D_ct=0\n",
    "    total_loss_D_mri=0\n",
    "    count=0\n",
    "    with tqdm(loader, total=len(loader), desc=f\"Epoch {epoch+1}/{params['num_epochs']}\") as pbar:\n",
    "        for mri_img,ct_img in pbar:\n",
    "            mri_img = mri_img.to(device)\n",
    "            ct_img = ct_img.to(device)\n",
    "\n",
    "            # 생성자 G와 F 업데이트\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # 아이덴티티 손실\n",
    "            loss_id_G = criterion_identity(G(ct_img), ct_img) * 5.0\n",
    "            loss_id_F = criterion_identity(F(mri_img), mri_img) * 5.0\n",
    "\n",
    "            # GAN 손실\n",
    "            fake_ct = G(mri_img)\n",
    "            pred_fake = D_ct(fake_ct)\n",
    "            loss_GAN_G = criterion_GAN(pred_fake, torch.ones_like(pred_fake).to(device))\n",
    "\n",
    "            fake_mri = F(ct_img)\n",
    "            pred_fake = D_mri(fake_mri)\n",
    "            loss_GAN_F = criterion_GAN(pred_fake, torch.ones_like(pred_fake).to(device))\n",
    "\n",
    "            # 순환 일관성 손실\n",
    "            recov_mri = F(fake_ct)\n",
    "            loss_cycle_GF = criterion_cycle(recov_mri, mri_img) * 10.0\n",
    "\n",
    "            recov_ct = G(fake_mri)\n",
    "            loss_cycle_FG = criterion_cycle(recov_ct, ct_img) * 10.0\n",
    "\n",
    "            # 총 생성자 손실\n",
    "            loss_G = loss_id_G + loss_id_F + loss_GAN_G + loss_GAN_F + loss_cycle_GF + loss_cycle_FG\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # 판별자 D_ct 업데이트\n",
    "            optimizer_D_ct.zero_grad()\n",
    "\n",
    "            pred_real = D_ct(ct_img)\n",
    "            loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real).to(device))\n",
    "\n",
    "            pred_fake = D_ct(fake_ct.detach())\n",
    "            loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake).to(device))\n",
    "\n",
    "            loss_D_ct = (loss_D_real + loss_D_fake) * 0.5\n",
    "            loss_D_ct.backward()\n",
    "            optimizer_D_ct.step()\n",
    "\n",
    "            # 판별자 D_mri 업데이트\n",
    "            optimizer_D_mri.zero_grad()\n",
    "\n",
    "            pred_real = D_mri(mri_img)\n",
    "            loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real).to(device))\n",
    "\n",
    "            pred_fake = D_mri(fake_mri.detach())\n",
    "            loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake).to(device))\n",
    "\n",
    "            loss_D_mri = (loss_D_real + loss_D_fake) * 0.5\n",
    "            loss_D_mri.backward()\n",
    "            optimizer_D_mri.step()\n",
    "            with torch.no_grad():\n",
    "                fake_ct = G(mri_img)\n",
    "                recov_mri = F(fake_ct)\n",
    "            def denormalize(img):\n",
    "                return img * 0.5 + 0.5\n",
    "\n",
    "            mri_img_vis = denormalize(mri_img[0])\n",
    "            ct_img_vis = denormalize(ct_img[0])\n",
    "            fake_ct_vis = denormalize(fake_ct[0])\n",
    "            recov_mri_vis = denormalize(recov_mri[0])\n",
    "\n",
    "            # 이미지 리스트 생성\n",
    "            images = [mri_img_vis, fake_ct_vis, recov_mri_vis, ct_img_vis]\n",
    "\n",
    "            # 이미지들을 너비 방향으로 concatenate\n",
    "            concatenated = torch.cat(images, dim=2)  # dim=3은 너비 방향\n",
    "            total_loss_D_ct+=loss_D_ct.item()\n",
    "            total_loss_D_mri+=loss_D_mri.item()\n",
    "            total_loss_G+=loss_G.item()\n",
    "            count+=1\n",
    "            pbar.set_postfix({\n",
    "                'Loss G': f'{total_loss_G/count:.4f}',\n",
    "                'Loss D_ct': f'{total_loss_D_ct/count:.4f}',\n",
    "                'Loss D_mri': f'{total_loss_D_mri/count:.4f}'\n",
    "            })\n",
    "\n",
    "    save_image(concatenated, f'../../result/translation/mri2ct/concatenated_epoch{epoch}.png')\n",
    "    torch.save(G.state_dict(), f'../../model/translation/mri2ct/G_{epoch}.pth')\n",
    "    torch.save(F.state_dict(), f'../../model/translation/mri2ct/F_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "G.load_state_dict(torch.load('G.pth'))\n",
    "G.eval()\n",
    "\n",
    "# 테스트 이미지 로드\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "from PIL import Image\n",
    "image = Image.open('path_to_grayscale_image.jpg')\n",
    "image = test_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "# 컬러화된 이미지 생성\n",
    "with torch.no_grad():\n",
    "    fake_color = G(image)\n",
    "\n",
    "# 이미지 저장\n",
    "save_image(fake_color, 'colorized_image.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
