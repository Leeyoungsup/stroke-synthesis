{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=5)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image\n",
    "import itertools\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image\n",
    "topilimage =transforms.ToPILImage()\n",
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model params\n",
    "params = {\n",
    "    'batch_size':64,\n",
    "    'input_size':128,\n",
    "    'resize_scale':128,\n",
    "    'crop_size':128,\n",
    "    'fliplr':False,\n",
    "    'num_epochs':100,\n",
    "    'decay_epoch':50,\n",
    "    'ngf':32,   #number of generator filters\n",
    "    'ndf':64,   #number of discriminator filters\n",
    "    'num_resnet':4, #number of resnet blocks\n",
    "    'lrG':2e-5,    #learning rate for generator\n",
    "    'lrD':2e-5,    #learning rate for discriminator\n",
    "    'beta1':0.5 ,    #beta1 for Adam optimizer\n",
    "    'beta2':0.999 ,  #beta2 for Adam optimizer\n",
    "    'lambdaA':10 ,   #lambdaA for cycle loss\n",
    "    'lambdaB':10,  #lambdaB for cycle loss\n",
    "    'img_form':'nii.gz'\n",
    "}\n",
    "\n",
    "data_dir = '../../data/origin_type/tar/**/**/'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14103/14103 [11:15<00:00, 20.89it/s]\n"
     ]
    }
   ],
   "source": [
    "def to_np(x):\n",
    "    return x.data.cpu().numpy()\n",
    "class ImagePool():\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "        if self.pool_size > 0:\n",
    "            self.num_imgs = 0\n",
    "            self.images = []\n",
    "\n",
    "    def query(self, images):\n",
    "        if self.pool_size == 0:\n",
    "            return images\n",
    "        return_images = []\n",
    "        with torch.no_grad():  # No need to track gradients\n",
    "            for image in images:\n",
    "                image = torch.unsqueeze(image, 0)\n",
    "                if self.num_imgs < self.pool_size:\n",
    "                    self.num_imgs += 1\n",
    "                    self.images.append(image)\n",
    "                    return_images.append(image)\n",
    "                else:\n",
    "                    p = random.uniform(0, 1)\n",
    "                    if p > 0.5:\n",
    "                        random_id = random.randint(0, self.pool_size - 1)\n",
    "                        tmp = self.images[random_id].clone()\n",
    "                        self.images[random_id] = image\n",
    "                        return_images.append(tmp)\n",
    "                    else:\n",
    "                        return_images.append(image)\n",
    "            return_images = torch.cat(return_images, 0)\n",
    "        return return_images.detach()  \n",
    "        \n",
    "class DatasetFromFolder(data.Dataset):\n",
    "    def __init__(self, gray_image_list,color_image_list):\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        self.gray_image_list =gray_image_list\n",
    "        self.color_image_list =color_image_list\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load Image\n",
    "        img = self.gray_image_list[index]*2-1\n",
    "        target = self.color_image_list[index]*2-1\n",
    "        return img, target\n",
    "    def __len__(self):\n",
    "        return len(self.gray_image_list)\n",
    "    \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=params['input_size']),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "#Subfolders - day & night\n",
    "data_a_list=glob(data_dir+'*.'+params['img_form'])\n",
    "data_b_list=[f.replace('/tar', '/nor') for f in data_a_list]\n",
    "gray_img_tensor=torch.zeros(len(data_a_list),3,params['input_size'],params['input_size'])\n",
    "color_img_tensor=torch.zeros(len(data_a_list),3,params['input_size'],params['input_size'])\n",
    "for i in tqdm(range(len(data_a_list))):\n",
    "    img=Image.open(data_a_list[i]).convert('L').convert('RGB')\n",
    "    img=transform(img)\n",
    "    gray_img_tensor[i]=img\n",
    "    img=Image.open(data_b_list[i]).convert('RGB')\n",
    "    img=transform(img)\n",
    "    color_img_tensor[i]=img\n",
    "train_data_A = DatasetFromFolder(gray_img_tensor,color_img_tensor)\n",
    "loader = torch.utils.data.DataLoader(dataset=train_data_A , batch_size=params['batch_size'], shuffle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CycleGAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(features, features, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(features, features, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(features),\n",
    "            nn.Dropout(0.5)  # Dropout 추가 (드롭아웃 확률 0.5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "# Generator Model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, n_residual_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "        # 초기 컨볼루션 블록\n",
    "        model = [\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=7, padding=3),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "\n",
    "        # 다운샘플링\n",
    "        in_features = 64\n",
    "        out_features = in_features * 2\n",
    "        for _ in range(4):  # 기존 2에서 4로 변경\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "\n",
    "        # 잔차 블록\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "\n",
    "        # 업샘플링\n",
    "        out_features = in_features // 2\n",
    "        for _ in range(4):  # 기존 2에서 4로 변경\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features // 2\n",
    "\n",
    "        # 출력 레이어\n",
    "        model += [nn.Conv2d(64, output_channels, kernel_size=7, padding=3), nn.Tanh()]\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Discriminator Model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, kernel_size=4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(input_channels, 64, normalization=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(3, 3).to(device)  # 그레이스케일에서 컬러로\n",
    "F = Generator(3, 3).to(device)  # 컬러에서 그레이스케일로\n",
    "D_color = Discriminator(3).to(device)\n",
    "D_gray = Discriminator(3).to(device)\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizer_G = optim.Adam(itertools.chain(G.parameters(), F.parameters()), lr=2e-5, betas=(0.5, 0.999))\n",
    "optimizer_D_color = optim.Adam(D_color.parameters(), lr=2e-5, betas=(0.5, 0.999))\n",
    "optimizer_D_gray = optim.Adam(D_gray.parameters(), lr=2e-5, betas=(0.5, 0.999))\n",
    "\n",
    "# 학습률 스케줄러 추가\n",
    "lr_scheduler_G = optim.lr_scheduler.StepLR(optimizer_G, step_size=20, gamma=0.5)\n",
    "lr_scheduler_D_color = optim.lr_scheduler.StepLR(optimizer_D_color, step_size=20, gamma=0.5)\n",
    "lr_scheduler_D_gray = optim.lr_scheduler.StepLR(optimizer_D_gray, step_size=20, gamma=0.5)\n",
    "\n",
    "# 손실 함수 설정\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 7052/7052 [2:41:40<00:00,  1.38s/it, Loss G=6.2058, Loss D_color=0.1726, Loss D_gray=0.2055]  \n",
      "Epoch 2/100: 100%|██████████| 7052/7052 [2:41:31<00:00,  1.37s/it, Loss G=5.0596, Loss D_color=0.1545, Loss D_gray=0.2287]  \n",
      "Epoch 3/100: 100%|██████████| 7052/7052 [2:41:27<00:00,  1.37s/it, Loss G=4.6391, Loss D_color=0.1580, Loss D_gray=0.2189]  \n",
      "Epoch 4/100: 100%|██████████| 7052/7052 [2:41:24<00:00,  1.37s/it, Loss G=4.3918, Loss D_color=0.1494, Loss D_gray=0.2010]  \n",
      "Epoch 5/100: 100%|██████████| 7052/7052 [2:41:21<00:00,  1.37s/it, Loss G=4.1983, Loss D_color=0.1368, Loss D_gray=0.2000]  \n",
      "Epoch 6/100: 100%|██████████| 7052/7052 [2:42:15<00:00,  1.38s/it, Loss G=4.1434, Loss D_color=0.1043, Loss D_gray=0.2038]  \n",
      "Epoch 7/100: 100%|██████████| 7052/7052 [2:42:35<00:00,  1.38s/it, Loss G=4.1292, Loss D_color=0.0829, Loss D_gray=0.1922]  \n",
      "Epoch 8/100: 100%|██████████| 7052/7052 [2:42:27<00:00,  1.38s/it, Loss G=4.0874, Loss D_color=0.0870, Loss D_gray=0.1706]  \n",
      "Epoch 9/100: 100%|██████████| 7052/7052 [2:42:28<00:00,  1.38s/it, Loss G=4.1210, Loss D_color=0.0968, Loss D_gray=0.1334]  \n",
      "Epoch 10/100: 100%|██████████| 7052/7052 [2:42:22<00:00,  1.38s/it, Loss G=4.1614, Loss D_color=0.0847, Loss D_gray=0.1118]  \n",
      "Epoch 11/100: 100%|██████████| 7052/7052 [2:42:11<00:00,  1.38s/it, Loss G=4.1906, Loss D_color=0.0812, Loss D_gray=0.1013]  \n",
      "Epoch 12/100: 100%|██████████| 7052/7052 [2:42:20<00:00,  1.38s/it, Loss G=4.1795, Loss D_color=0.0799, Loss D_gray=0.0953]  \n",
      "Epoch 13/100: 100%|██████████| 7052/7052 [2:42:20<00:00,  1.38s/it, Loss G=4.2019, Loss D_color=0.0703, Loss D_gray=0.0871]  \n",
      "Epoch 14/100: 100%|██████████| 7052/7052 [2:42:15<00:00,  1.38s/it, Loss G=4.1737, Loss D_color=0.0690, Loss D_gray=0.0863]  \n",
      "Epoch 15/100: 100%|██████████| 7052/7052 [4:44:40<00:00,  2.42s/it, Loss G=4.1362, Loss D_color=0.0662, Loss D_gray=0.0813]  \n",
      "Epoch 16/100: 100%|██████████| 7052/7052 [4:23:10<00:00,  2.24s/it, Loss G=4.1226, Loss D_color=0.0628, Loss D_gray=0.0780]  \n",
      "Epoch 17/100: 100%|██████████| 7052/7052 [2:42:09<00:00,  1.38s/it, Loss G=4.1502, Loss D_color=0.0547, Loss D_gray=0.0731]  \n",
      "Epoch 18/100: 100%|██████████| 7052/7052 [2:42:07<00:00,  1.38s/it, Loss G=4.0771, Loss D_color=0.0520, Loss D_gray=0.0724]  \n",
      "Epoch 19/100: 100%|██████████| 7052/7052 [2:42:02<00:00,  1.38s/it, Loss G=3.9999, Loss D_color=0.0549, Loss D_gray=0.0732]  \n",
      "Epoch 20/100: 100%|██████████| 7052/7052 [2:42:00<00:00,  1.38s/it, Loss G=3.9833, Loss D_color=0.0540, Loss D_gray=0.0707]  \n",
      "Epoch 21/100: 100%|██████████| 7052/7052 [2:41:58<00:00,  1.38s/it, Loss G=3.9467, Loss D_color=0.0576, Loss D_gray=0.0689]  \n",
      "Epoch 22/100: 100%|██████████| 7052/7052 [2:41:43<00:00,  1.38s/it, Loss G=4.0104, Loss D_color=0.0568, Loss D_gray=0.0660]  \n",
      "Epoch 23/100: 100%|██████████| 7052/7052 [2:42:26<00:00,  1.38s/it, Loss G=3.9449, Loss D_color=0.0590, Loss D_gray=0.0668]  \n",
      "Epoch 24/100:  25%|██▌       | 1763/7052 [40:25<2:01:16,  1.38s/it, Loss G=3.9512, Loss D_color=0.0534, Loss D_gray=0.0634]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# 총 생성자 손실\u001b[39;00m\n\u001b[1;32m     36\u001b[0m loss_G \u001b[38;5;241m=\u001b[39m loss_id_G \u001b[38;5;241m+\u001b[39m loss_id_F \u001b[38;5;241m+\u001b[39m loss_GAN_G \u001b[38;5;241m+\u001b[39m loss_GAN_F \u001b[38;5;241m+\u001b[39m loss_cycle_GF \u001b[38;5;241m+\u001b[39m loss_cycle_FG\n\u001b[0;32m---> 37\u001b[0m \u001b[43mloss_G\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m optimizer_G\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# 판별자 D_color 업데이트\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(params['num_epochs']):\n",
    "    \n",
    "    total_loss_G=0\n",
    "    total_loss_D_color=0\n",
    "    total_loss_D_gray=0\n",
    "    count=0\n",
    "    with tqdm(loader, total=len(loader), desc=f\"Epoch {epoch+1}/{params['num_epochs']}\") as pbar:\n",
    "        for gray_img,color_img in pbar:\n",
    "            gray_img = gray_img.to(device)\n",
    "            color_img = color_img.to(device)\n",
    "\n",
    "            # 생성자 G와 F 업데이트\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # 아이덴티티 손실\n",
    "            loss_id_G = criterion_identity(G(color_img), color_img) * 5.0\n",
    "            loss_id_F = criterion_identity(F(gray_img), gray_img) * 5.0\n",
    "\n",
    "            # GAN 손실\n",
    "            fake_color = G(gray_img)\n",
    "            pred_fake = D_color(fake_color)\n",
    "            loss_GAN_G = criterion_GAN(pred_fake, torch.ones_like(pred_fake).to(device))\n",
    "\n",
    "            fake_gray = F(color_img)\n",
    "            pred_fake = D_gray(fake_gray)\n",
    "            loss_GAN_F = criterion_GAN(pred_fake, torch.ones_like(pred_fake).to(device))\n",
    "\n",
    "            # 순환 일관성 손실\n",
    "            recov_gray = F(fake_color)\n",
    "            loss_cycle_GF = criterion_cycle(recov_gray, gray_img) * 10.0\n",
    "\n",
    "            recov_color = G(fake_gray)\n",
    "            loss_cycle_FG = criterion_cycle(recov_color, color_img) * 10.0\n",
    "\n",
    "            # 총 생성자 손실\n",
    "            loss_G = loss_id_G + loss_id_F + loss_GAN_G + loss_GAN_F + loss_cycle_GF + loss_cycle_FG\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # 판별자 D_color 업데이트\n",
    "            optimizer_D_color.zero_grad()\n",
    "\n",
    "            pred_real = D_color(color_img)\n",
    "            loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real).to(device))\n",
    "\n",
    "            pred_fake = D_color(fake_color.detach())\n",
    "            loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake).to(device))\n",
    "\n",
    "            loss_D_color = (loss_D_real + loss_D_fake) * 0.5\n",
    "            loss_D_color.backward()\n",
    "            optimizer_D_color.step()\n",
    "\n",
    "            # 판별자 D_gray 업데이트\n",
    "            optimizer_D_gray.zero_grad()\n",
    "\n",
    "            pred_real = D_gray(gray_img)\n",
    "            loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real).to(device))\n",
    "\n",
    "            pred_fake = D_gray(fake_gray.detach())\n",
    "            loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake).to(device))\n",
    "\n",
    "            loss_D_gray = (loss_D_real + loss_D_fake) * 0.5\n",
    "            loss_D_gray.backward()\n",
    "            optimizer_D_gray.step()\n",
    "            with torch.no_grad():\n",
    "                fake_color = G(gray_img)\n",
    "                recov_gray = F(fake_color)\n",
    "            def denormalize(img):\n",
    "                return img * 0.5 + 0.5\n",
    "\n",
    "            gray_img_vis = denormalize(gray_img[0])\n",
    "            color_img_vis = denormalize(color_img[0])\n",
    "            fake_color_vis = denormalize(fake_color[0])\n",
    "            recov_gray_vis = denormalize(recov_gray[0])\n",
    "\n",
    "            # 이미지 리스트 생성\n",
    "            images = [gray_img_vis, fake_color_vis, recov_gray_vis, color_img_vis]\n",
    "\n",
    "            # 이미지들을 너비 방향으로 concatenate\n",
    "            concatenated = torch.cat(images, dim=2)  # dim=3은 너비 방향\n",
    "            total_loss_D_color+=loss_D_color.item()\n",
    "            total_loss_D_gray+=loss_D_gray.item()\n",
    "            total_loss_G+=loss_G.item()\n",
    "            count+=1\n",
    "            pbar.set_postfix({\n",
    "                'Loss G': f'{total_loss_G/count:.4f}',\n",
    "                'Loss D_color': f'{total_loss_D_color/count:.4f}',\n",
    "                'Loss D_gray': f'{total_loss_D_gray/count:.4f}'\n",
    "            })\n",
    "\n",
    "    save_image(concatenated, f'../../result/colorization/cycleGAN/concatenated_epoch{epoch}.png')\n",
    "    torch.save(G.state_dict(), f'../../model/colorization/cycleGAN/G_{epoch}.pth')\n",
    "    torch.save(F.state_dict(), f'../../model/colorization/cycleGAN/F_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "G.load_state_dict(torch.load('G.pth'))\n",
    "G.eval()\n",
    "\n",
    "# 테스트 이미지 로드\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "from PIL import Image\n",
    "image = Image.open('path_to_grayscale_image.jpg')\n",
    "image = test_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "# 컬러화된 이미지 생성\n",
    "with torch.no_grad():\n",
    "    fake_color = G(image)\n",
    "\n",
    "# 이미지 저장\n",
    "save_image(fake_color, 'colorized_image.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
