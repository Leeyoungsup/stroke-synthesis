{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import save_image\n",
    "from torchinfo import summary\n",
    "from PIL import Image\n",
    "from edm2_pytorch.model import SongUNet, DhariwalUNet, VPPrecond, VEPrecond, iDDPMPrecond, EDMPrecond\n",
    "\n",
    "\n",
    "# 클래스 정의\n",
    "class_list = ['Normal','Ischemic','Hemorrhagic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = {\n",
    "    # 데이터 설정\n",
    "    'data_path': '../../data/2D_CT/',\n",
    "    'image_count': 10000,\n",
    "    'image_size': 256,\n",
    "    'inch': 1,\n",
    "    'outch': 1,\n",
    "\n",
    "    # 학습 설정\n",
    "    'lr': 2e-5,\n",
    "    'batch_size': 8,\n",
    "    'epochs': 10000,\n",
    "    'save_every': 10,\n",
    "    'save_path': '../../result/edm2/CT',\n",
    "\n",
    "    # EDM 샘플링 관련\n",
    "    'rho': 3.0,\n",
    "    'sigma_min': 0.002,\n",
    "    'sigma_max': 30.0,\n",
    "    'sigma_data': 0.5,\n",
    "    'threshold': 0.0,\n",
    "\n",
    "    # 모델 구조\n",
    "    'cdim': 32,                        # base channels\n",
    "    'channel_mult': [1, 2, 4,4],      # 채널 증가 비율\n",
    "    'attn_resolutions': [16],           # self-attention이 들어갈 해상도 (예: [16])\n",
    "    'layers_per_block': 2           # 각 레벨마다 residual block 수\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images into tensor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2849/2849 [00:25<00:00, 110.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# 변환 정의\n",
    "# trans = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n",
    "# ])\n",
    "trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "def transback(x):\n",
    "    return (x.clamp(-1, 1) + 1) * 0.5\n",
    "\n",
    "# 이미지 로드\n",
    "image_paths, image_labels = [], []\n",
    "for i, cname in enumerate(class_list):\n",
    "    paths = sorted(glob(os.path.join(params['data_path'], cname, '*.png')))[:params['image_count']]\n",
    "    image_paths.extend(paths)\n",
    "    image_labels.extend([i] * len(paths))\n",
    "\n",
    "N = len(image_paths)\n",
    "C, H, W = params['inch'], params['image_size'], params['image_size']\n",
    "train_images = torch.zeros((N, C, H, W), dtype=torch.float32)\n",
    "\n",
    "print(\"Loading images into tensor...\")\n",
    "for i, path in enumerate(tqdm(image_paths)):\n",
    "    img = Image.open(path).convert('L').resize((W, H))\n",
    "    \n",
    "    train_images[i] = trans(img)\n",
    "train_labels = torch.tensor(image_labels, dtype=torch.long)\n",
    "\n",
    "# 커스텀 Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.images[index]\n",
    "        lab = self.labels[index]\n",
    "        # if random.random() > 0.5:\n",
    "        #     img = transforms.functional.hflip(img)\n",
    "        # if random.random() > 0.5:\n",
    "        #     img = transforms.functional.vflip(img)\n",
    "        return img, lab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "def edm_sample_sigma(batch_size, rho, sigma_min, sigma_max, device):\n",
    "    t = torch.rand(batch_size, device=device)\n",
    "    sigmas = (sigma_max ** (1 / rho) + t * (sigma_min ** (1 / rho) - sigma_max ** (1 / rho))) ** rho\n",
    "    return sigmas\n",
    "\n",
    "def get_edm_sigma_schedule(sigma_min, sigma_max, rho, num_steps, device):\n",
    "    ramp = torch.linspace(0, 1, num_steps, device=device)\n",
    "    sigmas = (sigma_max ** (1 / rho) + ramp * (sigma_min ** (1 / rho) - sigma_max ** (1 / rho))) ** rho\n",
    "    return sigmas\n",
    "# DataLoader\n",
    "train_dataset = CustomDataset(train_images, train_labels)\n",
    "dataloader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True,drop_last=True)\n",
    "# 모델 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================================================================\n",
      "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Kernel Shape\n",
      "============================================================================================================================================\n",
      "EDMPrecond                               [1, 1, 256, 256]          [1, 1, 256, 256]          --                        --\n",
      "├─DhariwalUNet: 1-1                      [1, 1, 256, 256]          [1, 1, 256, 256]          --                        --\n",
      "│    └─PositionalEmbedding: 2-1          [1]                       [1, 32]                   --                        --\n",
      "│    └─Linear: 2-2                       [1, 32]                   [1, 128]                  4,224                     --\n",
      "│    └─Linear: 2-3                       [1, 128]                  [1, 128]                  16,512                    --\n",
      "│    └─Linear: 2-4                       [1, 3]                    [1, 128]                  384                       --\n",
      "│    └─ModuleDict: 2-5                   --                        --                        --                        --\n",
      "│    │    └─Conv2d: 3-1                  [1, 1, 256, 256]          [1, 32, 256, 256]         320                       --\n",
      "│    │    └─UNetBlock: 3-2               [1, 32, 256, 256]         [1, 32, 256, 256]         --                        --\n",
      "│    │    │    └─GroupNorm: 4-1          [1, 32, 256, 256]         [1, 32, 256, 256]         64                        --\n",
      "│    │    │    └─Conv2d: 4-2             [1, 32, 256, 256]         [1, 32, 256, 256]         9,248                     --\n",
      "│    │    │    └─Linear: 4-3             [1, 128]                  [1, 64]                   8,256                     --\n",
      "│    │    │    └─GroupNorm: 4-4          [1, 32, 256, 256]         [1, 32, 256, 256]         64                        --\n",
      "│    │    │    └─Conv2d: 4-5             [1, 32, 256, 256]         [1, 32, 256, 256]         9,248                     --\n",
      "│    │    └─UNetBlock: 3-3               [1, 32, 256, 256]         [1, 32, 256, 256]         --                        --\n",
      "│    │    │    └─GroupNorm: 4-6          [1, 32, 256, 256]         [1, 32, 256, 256]         64                        --\n",
      "│    │    │    └─Conv2d: 4-7             [1, 32, 256, 256]         [1, 32, 256, 256]         9,248                     --\n",
      "│    │    │    └─Linear: 4-8             [1, 128]                  [1, 64]                   8,256                     --\n",
      "│    │    │    └─GroupNorm: 4-9          [1, 32, 256, 256]         [1, 32, 256, 256]         64                        --\n",
      "│    │    │    └─Conv2d: 4-10            [1, 32, 256, 256]         [1, 32, 256, 256]         9,248                     --\n",
      "│    │    └─UNetBlock: 3-4               [1, 32, 256, 256]         [1, 32, 128, 128]         --                        --\n",
      "│    │    │    └─GroupNorm: 4-11         [1, 32, 256, 256]         [1, 32, 256, 256]         64                        --\n",
      "│    │    │    └─Conv2d: 4-12            [1, 32, 256, 256]         [1, 32, 128, 128]         9,248                     --\n",
      "│    │    │    └─Linear: 4-13            [1, 128]                  [1, 64]                   8,256                     --\n",
      "│    │    │    └─GroupNorm: 4-14         [1, 32, 128, 128]         [1, 32, 128, 128]         64                        --\n",
      "│    │    │    └─Conv2d: 4-15            [1, 32, 128, 128]         [1, 32, 128, 128]         9,248                     --\n",
      "│    │    │    └─Conv2d: 4-16            [1, 32, 256, 256]         [1, 32, 128, 128]         --                        --\n",
      "│    │    └─UNetBlock: 3-5               [1, 32, 128, 128]         [1, 64, 128, 128]         --                        --\n",
      "│    │    │    └─GroupNorm: 4-17         [1, 32, 128, 128]         [1, 32, 128, 128]         64                        --\n",
      "│    │    │    └─Conv2d: 4-18            [1, 32, 128, 128]         [1, 64, 128, 128]         18,496                    --\n",
      "│    │    │    └─Linear: 4-19            [1, 128]                  [1, 128]                  16,512                    --\n",
      "│    │    │    └─GroupNorm: 4-20         [1, 64, 128, 128]         [1, 64, 128, 128]         128                       --\n",
      "│    │    │    └─Conv2d: 4-21            [1, 64, 128, 128]         [1, 64, 128, 128]         36,928                    --\n",
      "│    │    │    └─Conv2d: 4-22            [1, 32, 128, 128]         [1, 64, 128, 128]         2,112                     --\n",
      "│    │    └─UNetBlock: 3-6               [1, 64, 128, 128]         [1, 64, 128, 128]         --                        --\n",
      "│    │    │    └─GroupNorm: 4-23         [1, 64, 128, 128]         [1, 64, 128, 128]         128                       --\n",
      "│    │    │    └─Conv2d: 4-24            [1, 64, 128, 128]         [1, 64, 128, 128]         36,928                    --\n",
      "│    │    │    └─Linear: 4-25            [1, 128]                  [1, 128]                  16,512                    --\n",
      "│    │    │    └─GroupNorm: 4-26         [1, 64, 128, 128]         [1, 64, 128, 128]         128                       --\n",
      "│    │    │    └─Conv2d: 4-27            [1, 64, 128, 128]         [1, 64, 128, 128]         36,928                    --\n",
      "│    │    └─UNetBlock: 3-7               [1, 64, 128, 128]         [1, 64, 64, 64]           --                        --\n",
      "│    │    │    └─GroupNorm: 4-28         [1, 64, 128, 128]         [1, 64, 128, 128]         128                       --\n",
      "│    │    │    └─Conv2d: 4-29            [1, 64, 128, 128]         [1, 64, 64, 64]           36,928                    --\n",
      "│    │    │    └─Linear: 4-30            [1, 128]                  [1, 128]                  16,512                    --\n",
      "│    │    │    └─GroupNorm: 4-31         [1, 64, 64, 64]           [1, 64, 64, 64]           128                       --\n",
      "│    │    │    └─Conv2d: 4-32            [1, 64, 64, 64]           [1, 64, 64, 64]           36,928                    --\n",
      "│    │    │    └─Conv2d: 4-33            [1, 64, 128, 128]         [1, 64, 64, 64]           --                        --\n",
      "│    │    └─UNetBlock: 3-8               [1, 64, 64, 64]           [1, 128, 64, 64]          --                        --\n",
      "│    │    │    └─GroupNorm: 4-34         [1, 64, 64, 64]           [1, 64, 64, 64]           128                       --\n",
      "│    │    │    └─Conv2d: 4-35            [1, 64, 64, 64]           [1, 128, 64, 64]          73,856                    --\n",
      "│    │    │    └─Linear: 4-36            [1, 128]                  [1, 256]                  33,024                    --\n",
      "│    │    │    └─GroupNorm: 4-37         [1, 128, 64, 64]          [1, 128, 64, 64]          256                       --\n",
      "│    │    │    └─Conv2d: 4-38            [1, 128, 64, 64]          [1, 128, 64, 64]          147,584                   --\n",
      "│    │    │    └─Conv2d: 4-39            [1, 64, 64, 64]           [1, 128, 64, 64]          8,320                     --\n",
      "│    │    └─UNetBlock: 3-9               [1, 128, 64, 64]          [1, 128, 64, 64]          --                        --\n",
      "│    │    │    └─GroupNorm: 4-40         [1, 128, 64, 64]          [1, 128, 64, 64]          256                       --\n",
      "│    │    │    └─Conv2d: 4-41            [1, 128, 64, 64]          [1, 128, 64, 64]          147,584                   --\n",
      "│    │    │    └─Linear: 4-42            [1, 128]                  [1, 256]                  33,024                    --\n",
      "│    │    │    └─GroupNorm: 4-43         [1, 128, 64, 64]          [1, 128, 64, 64]          256                       --\n",
      "│    │    │    └─Conv2d: 4-44            [1, 128, 64, 64]          [1, 128, 64, 64]          147,584                   --\n",
      "│    │    └─UNetBlock: 3-10              [1, 128, 64, 64]          [1, 128, 32, 32]          --                        --\n",
      "│    │    │    └─GroupNorm: 4-45         [1, 128, 64, 64]          [1, 128, 64, 64]          256                       --\n",
      "│    │    │    └─Conv2d: 4-46            [1, 128, 64, 64]          [1, 128, 32, 32]          147,584                   --\n",
      "│    │    │    └─Linear: 4-47            [1, 128]                  [1, 256]                  33,024                    --\n",
      "│    │    │    └─GroupNorm: 4-48         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
      "│    │    │    └─Conv2d: 4-49            [1, 128, 32, 32]          [1, 128, 32, 32]          147,584                   --\n",
      "│    │    │    └─Conv2d: 4-50            [1, 128, 64, 64]          [1, 128, 32, 32]          --                        --\n",
      "│    │    └─UNetBlock: 3-11              [1, 128, 32, 32]          [1, 128, 32, 32]          --                        --\n",
      "│    │    │    └─GroupNorm: 4-51         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
      "│    │    │    └─Conv2d: 4-52            [1, 128, 32, 32]          [1, 128, 32, 32]          147,584                   --\n",
      "│    │    │    └─Linear: 4-53            [1, 128]                  [1, 256]                  33,024                    --\n",
      "│    │    │    └─GroupNorm: 4-54         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
      "│    │    │    └─Conv2d: 4-55            [1, 128, 32, 32]          [1, 128, 32, 32]          147,584                   --\n",
      "│    │    └─UNetBlock: 3-12              [1, 128, 32, 32]          [1, 128, 32, 32]          --                        --\n",
      "│    │    │    └─GroupNorm: 4-56         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
      "│    │    │    └─Conv2d: 4-57            [1, 128, 32, 32]          [1, 128, 32, 32]          147,584                   --\n",
      "│    │    │    └─Linear: 4-58            [1, 128]                  [1, 256]                  33,024                    --\n",
      "│    │    │    └─GroupNorm: 4-59         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
      "│    │    │    └─Conv2d: 4-60            [1, 128, 32, 32]          [1, 128, 32, 32]          147,584                   --\n",
      "│    └─ModuleDict: 2-6                   --                        --                        --                        --\n",
      "│    │    └─UNetBlock: 3-13              [1, 128, 32, 32]          [1, 128, 32, 32]          --                        --\n",
      "│    │    │    └─GroupNorm: 4-61         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
      "│    │    │    └─Conv2d: 4-62            [1, 128, 32, 32]          [1, 128, 32, 32]          147,584                   --\n",
      "│    │    │    └─Linear: 4-63            [1, 128]                  [1, 256]                  33,024                    --\n",
      "│    │    │    └─GroupNorm: 4-64         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
      "│    │    │    └─Conv2d: 4-65            [1, 128, 32, 32]          [1, 128, 32, 32]          147,584                   --\n",
      "│    │    │    └─GroupNorm: 4-66         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
      "│    │    │    └─Conv2d: 4-67            [1, 128, 32, 32]          [1, 384, 32, 32]          49,536                    --\n",
      "│    │    │    └─Conv2d: 4-68            [1, 128, 32, 32]          [1, 128, 32, 32]          16,512                    --\n",
      "│    │    └─UNetBlock: 3-14              [1, 128, 32, 32]          [1, 128, 32, 32]          --                        --\n",
      "│    │    │    └─GroupNorm: 4-69         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
      "│    │    │    └─Conv2d: 4-70            [1, 128, 32, 32]          [1, 128, 32, 32]          147,584                   --\n",
      "│    │    │    └─Linear: 4-71            [1, 128]                  [1, 256]                  33,024                    --\n",
      "│    │    │    └─GroupNorm: 4-72         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
      "│    │    │    └─Conv2d: 4-73            [1, 128, 32, 32]          [1, 128, 32, 32]          147,584                   --\n",
      "│    │    └─UNetBlock: 3-15              [1, 256, 32, 32]          [1, 128, 32, 32]          --                        --\n",
      "│    │    │    └─GroupNorm: 4-74         [1, 256, 32, 32]          [1, 256, 32, 32]          512                       --\n",
      "│    │    │    └─Conv2d: 4-75            [1, 256, 32, 32]          [1, 128, 32, 32]          295,040                   --\n",
      "│    │    │    └─Linear: 4-76            [1, 128]                  [1, 256]                  33,024                    --\n",
      "│    │    │    └─GroupNorm: 4-77         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
      "│    │    │    └─Conv2d: 4-78            [1, 128, 32, 32]          [1, 128, 32, 32]          147,584                   --\n",
      "│    │    │    └─Conv2d: 4-79            [1, 256, 32, 32]          [1, 128, 32, 32]          32,896                    --\n",
      "│    │    └─UNetBlock: 3-16              [1, 256, 32, 32]          [1, 128, 32, 32]          --                        --\n",
      "│    │    │    └─GroupNorm: 4-80         [1, 256, 32, 32]          [1, 256, 32, 32]          512                       --\n",
      "│    │    │    └─Conv2d: 4-81            [1, 256, 32, 32]          [1, 128, 32, 32]          295,040                   --\n",
      "│    │    │    └─Linear: 4-82            [1, 128]                  [1, 256]                  33,024                    --\n",
      "│    │    │    └─GroupNorm: 4-83         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
      "│    │    │    └─Conv2d: 4-84            [1, 128, 32, 32]          [1, 128, 32, 32]          147,584                   --\n",
      "│    │    │    └─Conv2d: 4-85            [1, 256, 32, 32]          [1, 128, 32, 32]          32,896                    --\n",
      "│    │    └─UNetBlock: 3-17              [1, 256, 32, 32]          [1, 128, 32, 32]          --                        --\n",
      "│    │    │    └─GroupNorm: 4-86         [1, 256, 32, 32]          [1, 256, 32, 32]          512                       --\n",
      "│    │    │    └─Conv2d: 4-87            [1, 256, 32, 32]          [1, 128, 32, 32]          295,040                   --\n",
      "│    │    │    └─Linear: 4-88            [1, 128]                  [1, 256]                  33,024                    --\n",
      "│    │    │    └─GroupNorm: 4-89         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
      "│    │    │    └─Conv2d: 4-90            [1, 128, 32, 32]          [1, 128, 32, 32]          147,584                   --\n",
      "│    │    │    └─Conv2d: 4-91            [1, 256, 32, 32]          [1, 128, 32, 32]          32,896                    --\n",
      "│    │    └─UNetBlock: 3-18              [1, 128, 32, 32]          [1, 128, 64, 64]          --                        --\n",
      "│    │    │    └─GroupNorm: 4-92         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
      "│    │    │    └─Conv2d: 4-93            [1, 128, 32, 32]          [1, 128, 64, 64]          147,584                   --\n",
      "│    │    │    └─Linear: 4-94            [1, 128]                  [1, 256]                  33,024                    --\n",
      "│    │    │    └─GroupNorm: 4-95         [1, 128, 64, 64]          [1, 128, 64, 64]          256                       --\n",
      "│    │    │    └─Conv2d: 4-96            [1, 128, 64, 64]          [1, 128, 64, 64]          147,584                   --\n",
      "│    │    │    └─Conv2d: 4-97            [1, 128, 32, 32]          [1, 128, 64, 64]          --                        --\n",
      "│    │    └─UNetBlock: 3-19              [1, 256, 64, 64]          [1, 128, 64, 64]          --                        --\n",
      "│    │    │    └─GroupNorm: 4-98         [1, 256, 64, 64]          [1, 256, 64, 64]          512                       --\n",
      "│    │    │    └─Conv2d: 4-99            [1, 256, 64, 64]          [1, 128, 64, 64]          295,040                   --\n",
      "│    │    │    └─Linear: 4-100           [1, 128]                  [1, 256]                  33,024                    --\n",
      "│    │    │    └─GroupNorm: 4-101        [1, 128, 64, 64]          [1, 128, 64, 64]          256                       --\n",
      "│    │    │    └─Conv2d: 4-102           [1, 128, 64, 64]          [1, 128, 64, 64]          147,584                   --\n",
      "│    │    │    └─Conv2d: 4-103           [1, 256, 64, 64]          [1, 128, 64, 64]          32,896                    --\n",
      "│    │    └─UNetBlock: 3-20              [1, 256, 64, 64]          [1, 128, 64, 64]          --                        --\n",
      "│    │    │    └─GroupNorm: 4-104        [1, 256, 64, 64]          [1, 256, 64, 64]          512                       --\n",
      "│    │    │    └─Conv2d: 4-105           [1, 256, 64, 64]          [1, 128, 64, 64]          295,040                   --\n",
      "│    │    │    └─Linear: 4-106           [1, 128]                  [1, 256]                  33,024                    --\n",
      "│    │    │    └─GroupNorm: 4-107        [1, 128, 64, 64]          [1, 128, 64, 64]          256                       --\n",
      "│    │    │    └─Conv2d: 4-108           [1, 128, 64, 64]          [1, 128, 64, 64]          147,584                   --\n",
      "│    │    │    └─Conv2d: 4-109           [1, 256, 64, 64]          [1, 128, 64, 64]          32,896                    --\n",
      "│    │    └─UNetBlock: 3-21              [1, 192, 64, 64]          [1, 128, 64, 64]          --                        --\n",
      "│    │    │    └─GroupNorm: 4-110        [1, 192, 64, 64]          [1, 192, 64, 64]          384                       --\n",
      "│    │    │    └─Conv2d: 4-111           [1, 192, 64, 64]          [1, 128, 64, 64]          221,312                   --\n",
      "│    │    │    └─Linear: 4-112           [1, 128]                  [1, 256]                  33,024                    --\n",
      "│    │    │    └─GroupNorm: 4-113        [1, 128, 64, 64]          [1, 128, 64, 64]          256                       --\n",
      "│    │    │    └─Conv2d: 4-114           [1, 128, 64, 64]          [1, 128, 64, 64]          147,584                   --\n",
      "│    │    │    └─Conv2d: 4-115           [1, 192, 64, 64]          [1, 128, 64, 64]          24,704                    --\n",
      "│    │    └─UNetBlock: 3-22              [1, 128, 64, 64]          [1, 128, 128, 128]        --                        --\n",
      "│    │    │    └─GroupNorm: 4-116        [1, 128, 64, 64]          [1, 128, 64, 64]          256                       --\n",
      "│    │    │    └─Conv2d: 4-117           [1, 128, 64, 64]          [1, 128, 128, 128]        147,584                   --\n",
      "│    │    │    └─Linear: 4-118           [1, 128]                  [1, 256]                  33,024                    --\n",
      "│    │    │    └─GroupNorm: 4-119        [1, 128, 128, 128]        [1, 128, 128, 128]        256                       --\n",
      "│    │    │    └─Conv2d: 4-120           [1, 128, 128, 128]        [1, 128, 128, 128]        147,584                   --\n",
      "│    │    │    └─Conv2d: 4-121           [1, 128, 64, 64]          [1, 128, 128, 128]        --                        --\n",
      "│    │    └─UNetBlock: 3-23              [1, 192, 128, 128]        [1, 64, 128, 128]         --                        --\n",
      "│    │    │    └─GroupNorm: 4-122        [1, 192, 128, 128]        [1, 192, 128, 128]        384                       --\n",
      "│    │    │    └─Conv2d: 4-123           [1, 192, 128, 128]        [1, 64, 128, 128]         110,656                   --\n",
      "│    │    │    └─Linear: 4-124           [1, 128]                  [1, 128]                  16,512                    --\n",
      "│    │    │    └─GroupNorm: 4-125        [1, 64, 128, 128]         [1, 64, 128, 128]         128                       --\n",
      "│    │    │    └─Conv2d: 4-126           [1, 64, 128, 128]         [1, 64, 128, 128]         36,928                    --\n",
      "│    │    │    └─Conv2d: 4-127           [1, 192, 128, 128]        [1, 64, 128, 128]         12,352                    --\n",
      "│    │    └─UNetBlock: 3-24              [1, 128, 128, 128]        [1, 64, 128, 128]         --                        --\n",
      "│    │    │    └─GroupNorm: 4-128        [1, 128, 128, 128]        [1, 128, 128, 128]        256                       --\n",
      "│    │    │    └─Conv2d: 4-129           [1, 128, 128, 128]        [1, 64, 128, 128]         73,792                    --\n",
      "│    │    │    └─Linear: 4-130           [1, 128]                  [1, 128]                  16,512                    --\n",
      "│    │    │    └─GroupNorm: 4-131        [1, 64, 128, 128]         [1, 64, 128, 128]         128                       --\n",
      "│    │    │    └─Conv2d: 4-132           [1, 64, 128, 128]         [1, 64, 128, 128]         36,928                    --\n",
      "│    │    │    └─Conv2d: 4-133           [1, 128, 128, 128]        [1, 64, 128, 128]         8,256                     --\n",
      "│    │    └─UNetBlock: 3-25              [1, 96, 128, 128]         [1, 64, 128, 128]         --                        --\n",
      "│    │    │    └─GroupNorm: 4-134        [1, 96, 128, 128]         [1, 96, 128, 128]         192                       --\n",
      "│    │    │    └─Conv2d: 4-135           [1, 96, 128, 128]         [1, 64, 128, 128]         55,360                    --\n",
      "│    │    │    └─Linear: 4-136           [1, 128]                  [1, 128]                  16,512                    --\n",
      "│    │    │    └─GroupNorm: 4-137        [1, 64, 128, 128]         [1, 64, 128, 128]         128                       --\n",
      "│    │    │    └─Conv2d: 4-138           [1, 64, 128, 128]         [1, 64, 128, 128]         36,928                    --\n",
      "│    │    │    └─Conv2d: 4-139           [1, 96, 128, 128]         [1, 64, 128, 128]         6,208                     --\n",
      "│    │    └─UNetBlock: 3-26              [1, 64, 128, 128]         [1, 64, 256, 256]         --                        --\n",
      "│    │    │    └─GroupNorm: 4-140        [1, 64, 128, 128]         [1, 64, 128, 128]         128                       --\n",
      "│    │    │    └─Conv2d: 4-141           [1, 64, 128, 128]         [1, 64, 256, 256]         36,928                    --\n",
      "│    │    │    └─Linear: 4-142           [1, 128]                  [1, 128]                  16,512                    --\n",
      "│    │    │    └─GroupNorm: 4-143        [1, 64, 256, 256]         [1, 64, 256, 256]         128                       --\n",
      "│    │    │    └─Conv2d: 4-144           [1, 64, 256, 256]         [1, 64, 256, 256]         36,928                    --\n",
      "│    │    │    └─Conv2d: 4-145           [1, 64, 128, 128]         [1, 64, 256, 256]         --                        --\n",
      "│    │    └─UNetBlock: 3-27              [1, 96, 256, 256]         [1, 32, 256, 256]         --                        --\n",
      "│    │    │    └─GroupNorm: 4-146        [1, 96, 256, 256]         [1, 96, 256, 256]         192                       --\n",
      "│    │    │    └─Conv2d: 4-147           [1, 96, 256, 256]         [1, 32, 256, 256]         27,680                    --\n",
      "│    │    │    └─Linear: 4-148           [1, 128]                  [1, 64]                   8,256                     --\n",
      "│    │    │    └─GroupNorm: 4-149        [1, 32, 256, 256]         [1, 32, 256, 256]         64                        --\n",
      "│    │    │    └─Conv2d: 4-150           [1, 32, 256, 256]         [1, 32, 256, 256]         9,248                     --\n",
      "│    │    │    └─Conv2d: 4-151           [1, 96, 256, 256]         [1, 32, 256, 256]         3,104                     --\n",
      "│    │    └─UNetBlock: 3-28              [1, 64, 256, 256]         [1, 32, 256, 256]         --                        --\n",
      "│    │    │    └─GroupNorm: 4-152        [1, 64, 256, 256]         [1, 64, 256, 256]         128                       --\n",
      "│    │    │    └─Conv2d: 4-153           [1, 64, 256, 256]         [1, 32, 256, 256]         18,464                    --\n",
      "│    │    │    └─Linear: 4-154           [1, 128]                  [1, 64]                   8,256                     --\n",
      "│    │    │    └─GroupNorm: 4-155        [1, 32, 256, 256]         [1, 32, 256, 256]         64                        --\n",
      "│    │    │    └─Conv2d: 4-156           [1, 32, 256, 256]         [1, 32, 256, 256]         9,248                     --\n",
      "│    │    │    └─Conv2d: 4-157           [1, 64, 256, 256]         [1, 32, 256, 256]         2,080                     --\n",
      "│    │    └─UNetBlock: 3-29              [1, 64, 256, 256]         [1, 32, 256, 256]         --                        --\n",
      "│    │    │    └─GroupNorm: 4-158        [1, 64, 256, 256]         [1, 64, 256, 256]         128                       --\n",
      "│    │    │    └─Conv2d: 4-159           [1, 64, 256, 256]         [1, 32, 256, 256]         18,464                    --\n",
      "│    │    │    └─Linear: 4-160           [1, 128]                  [1, 64]                   8,256                     --\n",
      "│    │    │    └─GroupNorm: 4-161        [1, 32, 256, 256]         [1, 32, 256, 256]         64                        --\n",
      "│    │    │    └─Conv2d: 4-162           [1, 32, 256, 256]         [1, 32, 256, 256]         9,248                     --\n",
      "│    │    │    └─Conv2d: 4-163           [1, 64, 256, 256]         [1, 32, 256, 256]         2,080                     --\n",
      "│    └─GroupNorm: 2-7                    [1, 32, 256, 256]         [1, 32, 256, 256]         64                        --\n",
      "│    └─Conv2d: 2-8                       [1, 32, 256, 256]         [1, 1, 256, 256]          289                       --\n",
      "============================================================================================================================================\n",
      "Total params: 6,934,657\n",
      "Trainable params: 6,934,657\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 39.52\n",
      "============================================================================================================================================\n",
      "Input size (MB): 0.26\n",
      "Forward/backward pass size (MB): 1092.14\n",
      "Params size (MB): 27.74\n",
      "Estimated Total Size (MB): 1120.14\n",
      "============================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Kernel Shape\n",
       "============================================================================================================================================\n",
       "EDMPrecond                               [1, 1, 256, 256]          [1, 1, 256, 256]          --                        --\n",
       "├─DhariwalUNet: 1-1                      [1, 1, 256, 256]          [1, 1, 256, 256]          --                        --\n",
       "│    └─PositionalEmbedding: 2-1          [1]                       [1, 32]                   --                        --\n",
       "│    └─Linear: 2-2                       [1, 32]                   [1, 128]                  4,224                     --\n",
       "│    └─Linear: 2-3                       [1, 128]                  [1, 128]                  16,512                    --\n",
       "│    └─Linear: 2-4                       [1, 3]                    [1, 128]                  384                       --\n",
       "│    └─ModuleDict: 2-5                   --                        --                        --                        --\n",
       "│    │    └─Conv2d: 3-1                  [1, 1, 256, 256]          [1, 32, 256, 256]         320                       --\n",
       "│    │    └─UNetBlock: 3-2               [1, 32, 256, 256]         [1, 32, 256, 256]         --                        --\n",
       "│    │    │    └─GroupNorm: 4-1          [1, 32, 256, 256]         [1, 32, 256, 256]         64                        --\n",
       "│    │    │    └─Conv2d: 4-2             [1, 32, 256, 256]         [1, 32, 256, 256]         9,248                     --\n",
       "│    │    │    └─Linear: 4-3             [1, 128]                  [1, 64]                   8,256                     --\n",
       "│    │    │    └─GroupNorm: 4-4          [1, 32, 256, 256]         [1, 32, 256, 256]         64                        --\n",
       "│    │    │    └─Conv2d: 4-5             [1, 32, 256, 256]         [1, 32, 256, 256]         9,248                     --\n",
       "│    │    └─UNetBlock: 3-3               [1, 32, 256, 256]         [1, 32, 256, 256]         --                        --\n",
       "│    │    │    └─GroupNorm: 4-6          [1, 32, 256, 256]         [1, 32, 256, 256]         64                        --\n",
       "│    │    │    └─Conv2d: 4-7             [1, 32, 256, 256]         [1, 32, 256, 256]         9,248                     --\n",
       "│    │    │    └─Linear: 4-8             [1, 128]                  [1, 64]                   8,256                     --\n",
       "│    │    │    └─GroupNorm: 4-9          [1, 32, 256, 256]         [1, 32, 256, 256]         64                        --\n",
       "│    │    │    └─Conv2d: 4-10            [1, 32, 256, 256]         [1, 32, 256, 256]         9,248                     --\n",
       "│    │    └─UNetBlock: 3-4               [1, 32, 256, 256]         [1, 32, 128, 128]         --                        --\n",
       "│    │    │    └─GroupNorm: 4-11         [1, 32, 256, 256]         [1, 32, 256, 256]         64                        --\n",
       "│    │    │    └─Conv2d: 4-12            [1, 32, 256, 256]         [1, 32, 128, 128]         9,248                     --\n",
       "│    │    │    └─Linear: 4-13            [1, 128]                  [1, 64]                   8,256                     --\n",
       "│    │    │    └─GroupNorm: 4-14         [1, 32, 128, 128]         [1, 32, 128, 128]         64                        --\n",
       "│    │    │    └─Conv2d: 4-15            [1, 32, 128, 128]         [1, 32, 128, 128]         9,248                     --\n",
       "│    │    │    └─Conv2d: 4-16            [1, 32, 256, 256]         [1, 32, 128, 128]         --                        --\n",
       "│    │    └─UNetBlock: 3-5               [1, 32, 128, 128]         [1, 64, 128, 128]         --                        --\n",
       "│    │    │    └─GroupNorm: 4-17         [1, 32, 128, 128]         [1, 32, 128, 128]         64                        --\n",
       "│    │    │    └─Conv2d: 4-18            [1, 32, 128, 128]         [1, 64, 128, 128]         18,496                    --\n",
       "│    │    │    └─Linear: 4-19            [1, 128]                  [1, 128]                  16,512                    --\n",
       "│    │    │    └─GroupNorm: 4-20         [1, 64, 128, 128]         [1, 64, 128, 128]         128                       --\n",
       "│    │    │    └─Conv2d: 4-21            [1, 64, 128, 128]         [1, 64, 128, 128]         36,928                    --\n",
       "│    │    │    └─Conv2d: 4-22            [1, 32, 128, 128]         [1, 64, 128, 128]         2,112                     --\n",
       "│    │    └─UNetBlock: 3-6               [1, 64, 128, 128]         [1, 64, 128, 128]         --                        --\n",
       "│    │    │    └─GroupNorm: 4-23         [1, 64, 128, 128]         [1, 64, 128, 128]         128                       --\n",
       "│    │    │    └─Conv2d: 4-24            [1, 64, 128, 128]         [1, 64, 128, 128]         36,928                    --\n",
       "│    │    │    └─Linear: 4-25            [1, 128]                  [1, 128]                  16,512                    --\n",
       "│    │    │    └─GroupNorm: 4-26         [1, 64, 128, 128]         [1, 64, 128, 128]         128                       --\n",
       "│    │    │    └─Conv2d: 4-27            [1, 64, 128, 128]         [1, 64, 128, 128]         36,928                    --\n",
       "│    │    └─UNetBlock: 3-7               [1, 64, 128, 128]         [1, 64, 64, 64]           --                        --\n",
       "│    │    │    └─GroupNorm: 4-28         [1, 64, 128, 128]         [1, 64, 128, 128]         128                       --\n",
       "│    │    │    └─Conv2d: 4-29            [1, 64, 128, 128]         [1, 64, 64, 64]           36,928                    --\n",
       "│    │    │    └─Linear: 4-30            [1, 128]                  [1, 128]                  16,512                    --\n",
       "│    │    │    └─GroupNorm: 4-31         [1, 64, 64, 64]           [1, 64, 64, 64]           128                       --\n",
       "│    │    │    └─Conv2d: 4-32            [1, 64, 64, 64]           [1, 64, 64, 64]           36,928                    --\n",
       "│    │    │    └─Conv2d: 4-33            [1, 64, 128, 128]         [1, 64, 64, 64]           --                        --\n",
       "│    │    └─UNetBlock: 3-8               [1, 64, 64, 64]           [1, 128, 64, 64]          --                        --\n",
       "│    │    │    └─GroupNorm: 4-34         [1, 64, 64, 64]           [1, 64, 64, 64]           128                       --\n",
       "│    │    │    └─Conv2d: 4-35            [1, 64, 64, 64]           [1, 128, 64, 64]          73,856                    --\n",
       "│    │    │    └─Linear: 4-36            [1, 128]                  [1, 256]                  33,024                    --\n",
       "│    │    │    └─GroupNorm: 4-37         [1, 128, 64, 64]          [1, 128, 64, 64]          256                       --\n",
       "│    │    │    └─Conv2d: 4-38            [1, 128, 64, 64]          [1, 128, 64, 64]          147,584                   --\n",
       "│    │    │    └─Conv2d: 4-39            [1, 64, 64, 64]           [1, 128, 64, 64]          8,320                     --\n",
       "│    │    └─UNetBlock: 3-9               [1, 128, 64, 64]          [1, 128, 64, 64]          --                        --\n",
       "│    │    │    └─GroupNorm: 4-40         [1, 128, 64, 64]          [1, 128, 64, 64]          256                       --\n",
       "│    │    │    └─Conv2d: 4-41            [1, 128, 64, 64]          [1, 128, 64, 64]          147,584                   --\n",
       "│    │    │    └─Linear: 4-42            [1, 128]                  [1, 256]                  33,024                    --\n",
       "│    │    │    └─GroupNorm: 4-43         [1, 128, 64, 64]          [1, 128, 64, 64]          256                       --\n",
       "│    │    │    └─Conv2d: 4-44            [1, 128, 64, 64]          [1, 128, 64, 64]          147,584                   --\n",
       "│    │    └─UNetBlock: 3-10              [1, 128, 64, 64]          [1, 128, 32, 32]          --                        --\n",
       "│    │    │    └─GroupNorm: 4-45         [1, 128, 64, 64]          [1, 128, 64, 64]          256                       --\n",
       "│    │    │    └─Conv2d: 4-46            [1, 128, 64, 64]          [1, 128, 32, 32]          147,584                   --\n",
       "│    │    │    └─Linear: 4-47            [1, 128]                  [1, 256]                  33,024                    --\n",
       "│    │    │    └─GroupNorm: 4-48         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
       "│    │    │    └─Conv2d: 4-49            [1, 128, 32, 32]          [1, 128, 32, 32]          147,584                   --\n",
       "│    │    │    └─Conv2d: 4-50            [1, 128, 64, 64]          [1, 128, 32, 32]          --                        --\n",
       "│    │    └─UNetBlock: 3-11              [1, 128, 32, 32]          [1, 128, 32, 32]          --                        --\n",
       "│    │    │    └─GroupNorm: 4-51         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
       "│    │    │    └─Conv2d: 4-52            [1, 128, 32, 32]          [1, 128, 32, 32]          147,584                   --\n",
       "│    │    │    └─Linear: 4-53            [1, 128]                  [1, 256]                  33,024                    --\n",
       "│    │    │    └─GroupNorm: 4-54         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
       "│    │    │    └─Conv2d: 4-55            [1, 128, 32, 32]          [1, 128, 32, 32]          147,584                   --\n",
       "│    │    └─UNetBlock: 3-12              [1, 128, 32, 32]          [1, 128, 32, 32]          --                        --\n",
       "│    │    │    └─GroupNorm: 4-56         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
       "│    │    │    └─Conv2d: 4-57            [1, 128, 32, 32]          [1, 128, 32, 32]          147,584                   --\n",
       "│    │    │    └─Linear: 4-58            [1, 128]                  [1, 256]                  33,024                    --\n",
       "│    │    │    └─GroupNorm: 4-59         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
       "│    │    │    └─Conv2d: 4-60            [1, 128, 32, 32]          [1, 128, 32, 32]          147,584                   --\n",
       "│    └─ModuleDict: 2-6                   --                        --                        --                        --\n",
       "│    │    └─UNetBlock: 3-13              [1, 128, 32, 32]          [1, 128, 32, 32]          --                        --\n",
       "│    │    │    └─GroupNorm: 4-61         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
       "│    │    │    └─Conv2d: 4-62            [1, 128, 32, 32]          [1, 128, 32, 32]          147,584                   --\n",
       "│    │    │    └─Linear: 4-63            [1, 128]                  [1, 256]                  33,024                    --\n",
       "│    │    │    └─GroupNorm: 4-64         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
       "│    │    │    └─Conv2d: 4-65            [1, 128, 32, 32]          [1, 128, 32, 32]          147,584                   --\n",
       "│    │    │    └─GroupNorm: 4-66         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
       "│    │    │    └─Conv2d: 4-67            [1, 128, 32, 32]          [1, 384, 32, 32]          49,536                    --\n",
       "│    │    │    └─Conv2d: 4-68            [1, 128, 32, 32]          [1, 128, 32, 32]          16,512                    --\n",
       "│    │    └─UNetBlock: 3-14              [1, 128, 32, 32]          [1, 128, 32, 32]          --                        --\n",
       "│    │    │    └─GroupNorm: 4-69         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
       "│    │    │    └─Conv2d: 4-70            [1, 128, 32, 32]          [1, 128, 32, 32]          147,584                   --\n",
       "│    │    │    └─Linear: 4-71            [1, 128]                  [1, 256]                  33,024                    --\n",
       "│    │    │    └─GroupNorm: 4-72         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
       "│    │    │    └─Conv2d: 4-73            [1, 128, 32, 32]          [1, 128, 32, 32]          147,584                   --\n",
       "│    │    └─UNetBlock: 3-15              [1, 256, 32, 32]          [1, 128, 32, 32]          --                        --\n",
       "│    │    │    └─GroupNorm: 4-74         [1, 256, 32, 32]          [1, 256, 32, 32]          512                       --\n",
       "│    │    │    └─Conv2d: 4-75            [1, 256, 32, 32]          [1, 128, 32, 32]          295,040                   --\n",
       "│    │    │    └─Linear: 4-76            [1, 128]                  [1, 256]                  33,024                    --\n",
       "│    │    │    └─GroupNorm: 4-77         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
       "│    │    │    └─Conv2d: 4-78            [1, 128, 32, 32]          [1, 128, 32, 32]          147,584                   --\n",
       "│    │    │    └─Conv2d: 4-79            [1, 256, 32, 32]          [1, 128, 32, 32]          32,896                    --\n",
       "│    │    └─UNetBlock: 3-16              [1, 256, 32, 32]          [1, 128, 32, 32]          --                        --\n",
       "│    │    │    └─GroupNorm: 4-80         [1, 256, 32, 32]          [1, 256, 32, 32]          512                       --\n",
       "│    │    │    └─Conv2d: 4-81            [1, 256, 32, 32]          [1, 128, 32, 32]          295,040                   --\n",
       "│    │    │    └─Linear: 4-82            [1, 128]                  [1, 256]                  33,024                    --\n",
       "│    │    │    └─GroupNorm: 4-83         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
       "│    │    │    └─Conv2d: 4-84            [1, 128, 32, 32]          [1, 128, 32, 32]          147,584                   --\n",
       "│    │    │    └─Conv2d: 4-85            [1, 256, 32, 32]          [1, 128, 32, 32]          32,896                    --\n",
       "│    │    └─UNetBlock: 3-17              [1, 256, 32, 32]          [1, 128, 32, 32]          --                        --\n",
       "│    │    │    └─GroupNorm: 4-86         [1, 256, 32, 32]          [1, 256, 32, 32]          512                       --\n",
       "│    │    │    └─Conv2d: 4-87            [1, 256, 32, 32]          [1, 128, 32, 32]          295,040                   --\n",
       "│    │    │    └─Linear: 4-88            [1, 128]                  [1, 256]                  33,024                    --\n",
       "│    │    │    └─GroupNorm: 4-89         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
       "│    │    │    └─Conv2d: 4-90            [1, 128, 32, 32]          [1, 128, 32, 32]          147,584                   --\n",
       "│    │    │    └─Conv2d: 4-91            [1, 256, 32, 32]          [1, 128, 32, 32]          32,896                    --\n",
       "│    │    └─UNetBlock: 3-18              [1, 128, 32, 32]          [1, 128, 64, 64]          --                        --\n",
       "│    │    │    └─GroupNorm: 4-92         [1, 128, 32, 32]          [1, 128, 32, 32]          256                       --\n",
       "│    │    │    └─Conv2d: 4-93            [1, 128, 32, 32]          [1, 128, 64, 64]          147,584                   --\n",
       "│    │    │    └─Linear: 4-94            [1, 128]                  [1, 256]                  33,024                    --\n",
       "│    │    │    └─GroupNorm: 4-95         [1, 128, 64, 64]          [1, 128, 64, 64]          256                       --\n",
       "│    │    │    └─Conv2d: 4-96            [1, 128, 64, 64]          [1, 128, 64, 64]          147,584                   --\n",
       "│    │    │    └─Conv2d: 4-97            [1, 128, 32, 32]          [1, 128, 64, 64]          --                        --\n",
       "│    │    └─UNetBlock: 3-19              [1, 256, 64, 64]          [1, 128, 64, 64]          --                        --\n",
       "│    │    │    └─GroupNorm: 4-98         [1, 256, 64, 64]          [1, 256, 64, 64]          512                       --\n",
       "│    │    │    └─Conv2d: 4-99            [1, 256, 64, 64]          [1, 128, 64, 64]          295,040                   --\n",
       "│    │    │    └─Linear: 4-100           [1, 128]                  [1, 256]                  33,024                    --\n",
       "│    │    │    └─GroupNorm: 4-101        [1, 128, 64, 64]          [1, 128, 64, 64]          256                       --\n",
       "│    │    │    └─Conv2d: 4-102           [1, 128, 64, 64]          [1, 128, 64, 64]          147,584                   --\n",
       "│    │    │    └─Conv2d: 4-103           [1, 256, 64, 64]          [1, 128, 64, 64]          32,896                    --\n",
       "│    │    └─UNetBlock: 3-20              [1, 256, 64, 64]          [1, 128, 64, 64]          --                        --\n",
       "│    │    │    └─GroupNorm: 4-104        [1, 256, 64, 64]          [1, 256, 64, 64]          512                       --\n",
       "│    │    │    └─Conv2d: 4-105           [1, 256, 64, 64]          [1, 128, 64, 64]          295,040                   --\n",
       "│    │    │    └─Linear: 4-106           [1, 128]                  [1, 256]                  33,024                    --\n",
       "│    │    │    └─GroupNorm: 4-107        [1, 128, 64, 64]          [1, 128, 64, 64]          256                       --\n",
       "│    │    │    └─Conv2d: 4-108           [1, 128, 64, 64]          [1, 128, 64, 64]          147,584                   --\n",
       "│    │    │    └─Conv2d: 4-109           [1, 256, 64, 64]          [1, 128, 64, 64]          32,896                    --\n",
       "│    │    └─UNetBlock: 3-21              [1, 192, 64, 64]          [1, 128, 64, 64]          --                        --\n",
       "│    │    │    └─GroupNorm: 4-110        [1, 192, 64, 64]          [1, 192, 64, 64]          384                       --\n",
       "│    │    │    └─Conv2d: 4-111           [1, 192, 64, 64]          [1, 128, 64, 64]          221,312                   --\n",
       "│    │    │    └─Linear: 4-112           [1, 128]                  [1, 256]                  33,024                    --\n",
       "│    │    │    └─GroupNorm: 4-113        [1, 128, 64, 64]          [1, 128, 64, 64]          256                       --\n",
       "│    │    │    └─Conv2d: 4-114           [1, 128, 64, 64]          [1, 128, 64, 64]          147,584                   --\n",
       "│    │    │    └─Conv2d: 4-115           [1, 192, 64, 64]          [1, 128, 64, 64]          24,704                    --\n",
       "│    │    └─UNetBlock: 3-22              [1, 128, 64, 64]          [1, 128, 128, 128]        --                        --\n",
       "│    │    │    └─GroupNorm: 4-116        [1, 128, 64, 64]          [1, 128, 64, 64]          256                       --\n",
       "│    │    │    └─Conv2d: 4-117           [1, 128, 64, 64]          [1, 128, 128, 128]        147,584                   --\n",
       "│    │    │    └─Linear: 4-118           [1, 128]                  [1, 256]                  33,024                    --\n",
       "│    │    │    └─GroupNorm: 4-119        [1, 128, 128, 128]        [1, 128, 128, 128]        256                       --\n",
       "│    │    │    └─Conv2d: 4-120           [1, 128, 128, 128]        [1, 128, 128, 128]        147,584                   --\n",
       "│    │    │    └─Conv2d: 4-121           [1, 128, 64, 64]          [1, 128, 128, 128]        --                        --\n",
       "│    │    └─UNetBlock: 3-23              [1, 192, 128, 128]        [1, 64, 128, 128]         --                        --\n",
       "│    │    │    └─GroupNorm: 4-122        [1, 192, 128, 128]        [1, 192, 128, 128]        384                       --\n",
       "│    │    │    └─Conv2d: 4-123           [1, 192, 128, 128]        [1, 64, 128, 128]         110,656                   --\n",
       "│    │    │    └─Linear: 4-124           [1, 128]                  [1, 128]                  16,512                    --\n",
       "│    │    │    └─GroupNorm: 4-125        [1, 64, 128, 128]         [1, 64, 128, 128]         128                       --\n",
       "│    │    │    └─Conv2d: 4-126           [1, 64, 128, 128]         [1, 64, 128, 128]         36,928                    --\n",
       "│    │    │    └─Conv2d: 4-127           [1, 192, 128, 128]        [1, 64, 128, 128]         12,352                    --\n",
       "│    │    └─UNetBlock: 3-24              [1, 128, 128, 128]        [1, 64, 128, 128]         --                        --\n",
       "│    │    │    └─GroupNorm: 4-128        [1, 128, 128, 128]        [1, 128, 128, 128]        256                       --\n",
       "│    │    │    └─Conv2d: 4-129           [1, 128, 128, 128]        [1, 64, 128, 128]         73,792                    --\n",
       "│    │    │    └─Linear: 4-130           [1, 128]                  [1, 128]                  16,512                    --\n",
       "│    │    │    └─GroupNorm: 4-131        [1, 64, 128, 128]         [1, 64, 128, 128]         128                       --\n",
       "│    │    │    └─Conv2d: 4-132           [1, 64, 128, 128]         [1, 64, 128, 128]         36,928                    --\n",
       "│    │    │    └─Conv2d: 4-133           [1, 128, 128, 128]        [1, 64, 128, 128]         8,256                     --\n",
       "│    │    └─UNetBlock: 3-25              [1, 96, 128, 128]         [1, 64, 128, 128]         --                        --\n",
       "│    │    │    └─GroupNorm: 4-134        [1, 96, 128, 128]         [1, 96, 128, 128]         192                       --\n",
       "│    │    │    └─Conv2d: 4-135           [1, 96, 128, 128]         [1, 64, 128, 128]         55,360                    --\n",
       "│    │    │    └─Linear: 4-136           [1, 128]                  [1, 128]                  16,512                    --\n",
       "│    │    │    └─GroupNorm: 4-137        [1, 64, 128, 128]         [1, 64, 128, 128]         128                       --\n",
       "│    │    │    └─Conv2d: 4-138           [1, 64, 128, 128]         [1, 64, 128, 128]         36,928                    --\n",
       "│    │    │    └─Conv2d: 4-139           [1, 96, 128, 128]         [1, 64, 128, 128]         6,208                     --\n",
       "│    │    └─UNetBlock: 3-26              [1, 64, 128, 128]         [1, 64, 256, 256]         --                        --\n",
       "│    │    │    └─GroupNorm: 4-140        [1, 64, 128, 128]         [1, 64, 128, 128]         128                       --\n",
       "│    │    │    └─Conv2d: 4-141           [1, 64, 128, 128]         [1, 64, 256, 256]         36,928                    --\n",
       "│    │    │    └─Linear: 4-142           [1, 128]                  [1, 128]                  16,512                    --\n",
       "│    │    │    └─GroupNorm: 4-143        [1, 64, 256, 256]         [1, 64, 256, 256]         128                       --\n",
       "│    │    │    └─Conv2d: 4-144           [1, 64, 256, 256]         [1, 64, 256, 256]         36,928                    --\n",
       "│    │    │    └─Conv2d: 4-145           [1, 64, 128, 128]         [1, 64, 256, 256]         --                        --\n",
       "│    │    └─UNetBlock: 3-27              [1, 96, 256, 256]         [1, 32, 256, 256]         --                        --\n",
       "│    │    │    └─GroupNorm: 4-146        [1, 96, 256, 256]         [1, 96, 256, 256]         192                       --\n",
       "│    │    │    └─Conv2d: 4-147           [1, 96, 256, 256]         [1, 32, 256, 256]         27,680                    --\n",
       "│    │    │    └─Linear: 4-148           [1, 128]                  [1, 64]                   8,256                     --\n",
       "│    │    │    └─GroupNorm: 4-149        [1, 32, 256, 256]         [1, 32, 256, 256]         64                        --\n",
       "│    │    │    └─Conv2d: 4-150           [1, 32, 256, 256]         [1, 32, 256, 256]         9,248                     --\n",
       "│    │    │    └─Conv2d: 4-151           [1, 96, 256, 256]         [1, 32, 256, 256]         3,104                     --\n",
       "│    │    └─UNetBlock: 3-28              [1, 64, 256, 256]         [1, 32, 256, 256]         --                        --\n",
       "│    │    │    └─GroupNorm: 4-152        [1, 64, 256, 256]         [1, 64, 256, 256]         128                       --\n",
       "│    │    │    └─Conv2d: 4-153           [1, 64, 256, 256]         [1, 32, 256, 256]         18,464                    --\n",
       "│    │    │    └─Linear: 4-154           [1, 128]                  [1, 64]                   8,256                     --\n",
       "│    │    │    └─GroupNorm: 4-155        [1, 32, 256, 256]         [1, 32, 256, 256]         64                        --\n",
       "│    │    │    └─Conv2d: 4-156           [1, 32, 256, 256]         [1, 32, 256, 256]         9,248                     --\n",
       "│    │    │    └─Conv2d: 4-157           [1, 64, 256, 256]         [1, 32, 256, 256]         2,080                     --\n",
       "│    │    └─UNetBlock: 3-29              [1, 64, 256, 256]         [1, 32, 256, 256]         --                        --\n",
       "│    │    │    └─GroupNorm: 4-158        [1, 64, 256, 256]         [1, 64, 256, 256]         128                       --\n",
       "│    │    │    └─Conv2d: 4-159           [1, 64, 256, 256]         [1, 32, 256, 256]         18,464                    --\n",
       "│    │    │    └─Linear: 4-160           [1, 128]                  [1, 64]                   8,256                     --\n",
       "│    │    │    └─GroupNorm: 4-161        [1, 32, 256, 256]         [1, 32, 256, 256]         64                        --\n",
       "│    │    │    └─Conv2d: 4-162           [1, 32, 256, 256]         [1, 32, 256, 256]         9,248                     --\n",
       "│    │    │    └─Conv2d: 4-163           [1, 64, 256, 256]         [1, 32, 256, 256]         2,080                     --\n",
       "│    └─GroupNorm: 2-7                    [1, 32, 256, 256]         [1, 32, 256, 256]         64                        --\n",
       "│    └─Conv2d: 2-8                       [1, 32, 256, 256]         [1, 1, 256, 256]          289                       --\n",
       "============================================================================================================================================\n",
       "Total params: 6,934,657\n",
       "Trainable params: 6,934,657\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 39.52\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.26\n",
       "Forward/backward pass size (MB): 1092.14\n",
       "Params size (MB): 27.74\n",
       "Estimated Total Size (MB): 1120.14\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "model = EDMPrecond(\n",
    "    img_resolution=params['image_size'],\n",
    "    img_channels=params['inch'],\n",
    "    label_dim=len(class_list),\n",
    "    use_fp16=False,\n",
    "    sigma_min=params['sigma_min'],\n",
    "    sigma_max=params['sigma_max'],\n",
    "    sigma_data=params['sigma_data'],\n",
    "    model_type='DhariwalUNet',  # 또는 'SongUNet'\n",
    "    model_channels=params['cdim'],\n",
    "    channel_mult=params['channel_mult'],\n",
    "    channel_mult_emb=4,\n",
    "    num_blocks=params['layers_per_block'],\n",
    "    attn_resolutions=params['attn_resolutions'],\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=params['lr'])\n",
    "#모델 불러오기\n",
    "# model.load_state_dict(torch.load('../../model/edm2/CT/model_epoch_41.pt'))\n",
    "# 모델 요약\n",
    "summary(\n",
    "    model,\n",
    "    input_data=(\n",
    "        torch.randn(1, params['inch'], params['image_size'], params['image_size']).to(device),  # noised input\n",
    "        torch.tensor([params['sigma_data']], device=device),  # sigma\n",
    "        torch.nn.functional.one_hot(torch.tensor([0]), num_classes=len(class_list)).float().to(device)  # dummy class label\n",
    "    ),\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\"],\n",
    "    depth=4,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000: 100%|██████████| 356/356 [00:38<00:00,  9.22it/s, loss=0.2792]\n",
      "Sampling: 100%|██████████| 49/49 [00:01<00:00, 35.40it/s]\n",
      "Epoch 2/10000: 100%|██████████| 356/356 [00:38<00:00,  9.37it/s, loss=0.0945]\n",
      "Epoch 3/10000: 100%|██████████| 356/356 [00:38<00:00,  9.35it/s, loss=0.0733]\n",
      "Epoch 4/10000: 100%|██████████| 356/356 [00:37<00:00,  9.37it/s, loss=0.0675]\n",
      "Epoch 5/10000: 100%|██████████| 356/356 [00:37<00:00,  9.37it/s, loss=0.0653]\n",
      "Epoch 6/10000: 100%|██████████| 356/356 [00:37<00:00,  9.40it/s, loss=0.0609]\n",
      "Epoch 7/10000: 100%|██████████| 356/356 [00:37<00:00,  9.41it/s, loss=0.0566]\n",
      "Epoch 8/10000: 100%|██████████| 356/356 [00:37<00:00,  9.48it/s, loss=0.0573]\n",
      "Epoch 9/10000: 100%|██████████| 356/356 [00:37<00:00,  9.48it/s, loss=0.0568]\n",
      "Epoch 10/10000: 100%|██████████| 356/356 [00:37<00:00,  9.45it/s, loss=0.0540]\n",
      "Epoch 11/10000: 100%|██████████| 356/356 [00:37<00:00,  9.46it/s, loss=0.0542]\n",
      "Sampling: 100%|██████████| 49/49 [00:01<00:00, 36.98it/s]\n",
      "Epoch 12/10000: 100%|██████████| 356/356 [00:37<00:00,  9.46it/s, loss=0.0522]\n",
      "Epoch 13/10000: 100%|██████████| 356/356 [00:37<00:00,  9.46it/s, loss=0.0502]\n",
      "Epoch 14/10000: 100%|██████████| 356/356 [00:37<00:00,  9.47it/s, loss=0.0506]\n",
      "Epoch 15/10000: 100%|██████████| 356/356 [00:37<00:00,  9.44it/s, loss=0.0485]\n",
      "Epoch 16/10000: 100%|██████████| 356/356 [00:37<00:00,  9.47it/s, loss=0.0474]\n",
      "Epoch 17/10000: 100%|██████████| 356/356 [00:37<00:00,  9.46it/s, loss=0.0468]\n",
      "Epoch 18/10000: 100%|██████████| 356/356 [00:37<00:00,  9.45it/s, loss=0.0466]\n",
      "Epoch 19/10000: 100%|██████████| 356/356 [00:37<00:00,  9.46it/s, loss=0.0469]\n",
      "Epoch 20/10000: 100%|██████████| 356/356 [00:37<00:00,  9.47it/s, loss=0.0471]\n",
      "Epoch 21/10000: 100%|██████████| 356/356 [00:37<00:00,  9.46it/s, loss=0.0448]\n",
      "Sampling: 100%|██████████| 49/49 [00:01<00:00, 37.00it/s]\n",
      "Epoch 22/10000: 100%|██████████| 356/356 [00:37<00:00,  9.47it/s, loss=0.0446]\n",
      "Epoch 23/10000: 100%|██████████| 356/356 [00:37<00:00,  9.46it/s, loss=0.0455]\n",
      "Epoch 24/10000: 100%|██████████| 356/356 [00:37<00:00,  9.47it/s, loss=0.0448]\n",
      "Epoch 25/10000: 100%|██████████| 356/356 [00:37<00:00,  9.45it/s, loss=0.0444]\n",
      "Epoch 26/10000: 100%|██████████| 356/356 [00:37<00:00,  9.47it/s, loss=0.0438]\n",
      "Epoch 27/10000: 100%|██████████| 356/356 [00:37<00:00,  9.47it/s, loss=0.0439]\n",
      "Epoch 28/10000: 100%|██████████| 356/356 [00:37<00:00,  9.43it/s, loss=0.0441]\n",
      "Epoch 29/10000: 100%|██████████| 356/356 [00:37<00:00,  9.45it/s, loss=0.0443]\n",
      "Epoch 30/10000: 100%|██████████| 356/356 [00:37<00:00,  9.47it/s, loss=0.0445]\n",
      "Epoch 31/10000: 100%|██████████| 356/356 [00:37<00:00,  9.48it/s, loss=0.0429]\n",
      "Sampling: 100%|██████████| 49/49 [00:01<00:00, 37.01it/s]\n",
      "Epoch 32/10000: 100%|██████████| 356/356 [00:37<00:00,  9.48it/s, loss=0.0434]\n",
      "Epoch 33/10000: 100%|██████████| 356/356 [00:37<00:00,  9.48it/s, loss=0.0439]\n",
      "Epoch 34/10000: 100%|██████████| 356/356 [00:37<00:00,  9.46it/s, loss=0.0438]\n",
      "Epoch 35/10000: 100%|██████████| 356/356 [00:37<00:00,  9.48it/s, loss=0.0440]\n",
      "Epoch 36/10000: 100%|██████████| 356/356 [00:37<00:00,  9.48it/s, loss=0.0434]\n",
      "Epoch 37/10000: 100%|██████████| 356/356 [00:37<00:00,  9.45it/s, loss=0.0416]\n",
      "Epoch 38/10000: 100%|██████████| 356/356 [00:37<00:00,  9.41it/s, loss=0.0436]\n",
      "Epoch 39/10000: 100%|██████████| 356/356 [00:37<00:00,  9.48it/s, loss=0.0415]\n",
      "Epoch 40/10000: 100%|██████████| 356/356 [00:37<00:00,  9.45it/s, loss=0.0417]\n",
      "Epoch 41/10000: 100%|██████████| 356/356 [00:38<00:00,  9.35it/s, loss=0.0418]\n",
      "Sampling: 100%|██████████| 49/49 [00:01<00:00, 37.01it/s]\n",
      "Epoch 42/10000: 100%|██████████| 356/356 [00:37<00:00,  9.40it/s, loss=0.0426]\n",
      "Epoch 43/10000: 100%|██████████| 356/356 [00:37<00:00,  9.37it/s, loss=0.0442]\n",
      "Epoch 44/10000: 100%|██████████| 356/356 [00:37<00:00,  9.46it/s, loss=0.0424]\n",
      "Epoch 45/10000: 100%|██████████| 356/356 [00:37<00:00,  9.39it/s, loss=0.0432]\n",
      "Epoch 46/10000: 100%|██████████| 356/356 [00:38<00:00,  9.25it/s, loss=0.0426]\n",
      "Epoch 47/10000: 100%|██████████| 356/356 [00:37<00:00,  9.40it/s, loss=0.0438]\n",
      "Epoch 48/10000: 100%|██████████| 356/356 [00:37<00:00,  9.46it/s, loss=0.0412]\n",
      "Epoch 49/10000: 100%|██████████| 356/356 [00:37<00:00,  9.37it/s, loss=0.0432]\n",
      "Epoch 50/10000: 100%|██████████| 356/356 [00:37<00:00,  9.44it/s, loss=0.0412]\n",
      "Epoch 51/10000: 100%|██████████| 356/356 [00:37<00:00,  9.41it/s, loss=0.0430]\n",
      "Sampling: 100%|██████████| 49/49 [00:01<00:00, 36.99it/s]\n",
      "Epoch 52/10000:  95%|█████████▌| 339/356 [00:36<00:01,  9.37it/s, loss=0.0422]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m class_onehot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mone_hot(labels, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(class_list))\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# 모델 forward 및 손실 계산\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m denoised \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoised\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigmas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_onehot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m target \u001b[38;5;241m=\u001b[39m imgs\n\u001b[1;32m     32\u001b[0m l1 \u001b[38;5;241m=\u001b[39m (denoised \u001b[38;5;241m-\u001b[39m target)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/gcubme_ai2/Workspace/YS_Lee/stroke synthesis/code/stroke-synthesis/edm2_pytorch/model.py:653\u001b[0m, in \u001b[0;36mEDMPrecond.forward\u001b[0;34m(self, x, sigma, class_labels, force_fp32, **model_kwargs)\u001b[0m\n\u001b[1;32m    650\u001b[0m c_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigma_data \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m sigma \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39msqrt()\n\u001b[1;32m    651\u001b[0m c_noise \u001b[38;5;241m=\u001b[39m sigma\u001b[38;5;241m.\u001b[39mlog() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m--> 653\u001b[0m F_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_in\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_noise\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m F_x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m dtype\n\u001b[1;32m    655\u001b[0m D_x \u001b[38;5;241m=\u001b[39m c_skip \u001b[38;5;241m*\u001b[39m x \u001b[38;5;241m+\u001b[39m c_out \u001b[38;5;241m*\u001b[39m F_x\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/gcubme_ai2/Workspace/YS_Lee/stroke synthesis/code/stroke-synthesis/edm2_pytorch/model.py:451\u001b[0m, in \u001b[0;36mDhariwalUNet.forward\u001b[0;34m(self, x, noise_labels, class_labels, augment_labels)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m block\u001b[38;5;241m.\u001b[39min_channels:\n\u001b[1;32m    450\u001b[0m         x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, skips\u001b[38;5;241m.\u001b[39mpop()], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 451\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_conv(silu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_norm(x)))\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/gcubme_ai2/Workspace/YS_Lee/stroke synthesis/code/stroke-synthesis/edm2_pytorch/model.py:173\u001b[0m, in \u001b[0;36mUNetBlock.forward\u001b[0;34m(self, x, emb)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m     x \u001b[38;5;241m=\u001b[39m silu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x\u001b[38;5;241m.\u001b[39madd_(params)))\n\u001b[0;32m--> 173\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    174\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39madd_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip(orig) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m orig)\n\u001b[1;32m    175\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_scale\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.10/site-packages/torch/nn/functional.py:1295\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m(\u001b[38;5;28minput\u001b[39m, p, training)\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.10/site-packages/torch/_VF.py:26\u001b[0m, in \u001b[0;36mVFModule.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(name)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_VariableFunctions\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvf, attr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scaler = torch.cuda.amp.GradScaler()\n",
    "num_steps = 50\n",
    "shared_sigmas = get_edm_sigma_schedule(\n",
    "    sigma_min=params['sigma_min'],\n",
    "    sigma_max=params['sigma_max'],\n",
    "    rho=params['rho'],\n",
    "    num_steps=num_steps,\n",
    "    device=device\n",
    ")\n",
    "for epoch in range(1, params['epochs'] + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch}/{params['epochs']}\")\n",
    "    for step, (imgs, labels) in enumerate(pbar, start=1):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        # EDM 논문 공식 σ 샘플링 방식 (log-normal)\n",
    "        sigmas = shared_sigmas[torch.randint(0, num_steps, (imgs.shape[0],))]  # [B]\n",
    "\n",
    "        # 노이즈 추가\n",
    "        noise = torch.randn_like(imgs)\n",
    "        noised = imgs + sigmas.view(-1, 1, 1, 1) * noise\n",
    "\n",
    "        # 클래스 one-hot encoding\n",
    "        class_onehot = torch.nn.functional.one_hot(labels, num_classes=len(class_list)).float()\n",
    "\n",
    "        # 모델 forward 및 손실 계산\n",
    "        \n",
    "        denoised = model(noised, sigmas, class_labels=class_onehot)\n",
    "        target = imgs\n",
    "        l1 = (denoised - target).abs().mean()\n",
    "        l2 = torch.nn.functional.mse_loss(denoised, target)\n",
    "        # loss = 0.8 * l1 + 0.2 * l2\n",
    "        alpha = min(1.0, epoch / 1000)  # 0에서 시작해 1000 epoch에 1.0 도달\n",
    "        loss = alpha * l1 + (1 - alpha) * l2\n",
    "        # 역전파 및 업데이트\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / step\n",
    "        pbar.set_postfix(loss=f\"{avg_loss:.4f}\")\n",
    "\n",
    "    \n",
    "        # 주기적으로 샘플 저장\n",
    "    if (epoch - 1) % params['save_every'] == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            # 각 클래스별 동일 개수로 label 생성\n",
    "            num_per_class = params['batch_size'] // len(class_list)\n",
    "            label_list = []\n",
    "            for i in range(len(class_list)):\n",
    "                label_list.extend([i] * num_per_class)\n",
    "            label_tensor = torch.tensor(label_list, device=device)\n",
    "            class_onehot = torch.nn.functional.one_hot(label_tensor, num_classes=len(class_list)).float()\n",
    "\n",
    "            # 입력 noise 생성\n",
    "            z = torch.randn(len(label_tensor), params['inch'], params['image_size'], params['image_size']).to(device)\n",
    "            z = z * shared_sigmas[0].view(1, 1, 1, 1)\n",
    "\n",
    "            # 샘플링 루프: Euler-style\n",
    "            sigma = torch.full((z.shape[0], 1, 1, 1), params['sigma_max'], device=device)\n",
    "            for _ in tqdm(range(num_steps-1), desc=\"Sampling\"):\n",
    "                sigma = shared_sigmas[i].view(1, 1, 1, 1)\n",
    "                sigma_next = shared_sigmas[i + 1].view(1, 1, 1, 1)\n",
    "                denoised = model(z, sigma, class_labels=class_onehot)\n",
    "                d = (z - denoised) / sigma\n",
    "                dt = sigma_next - sigma\n",
    "                z = z + d * dt\n",
    "                sigma = sigma + dt\n",
    "                sigma = sigma.clamp(min=params['sigma_min'])\n",
    "\n",
    "            samples = transback(z).clamp(0,1)\n",
    "            save_image(samples, os.path.join(params['save_path'], f'sample_epoch_{epoch}.png'), nrow=num_per_class,normalize=True)\n",
    "            torch.save(model.state_dict(), os.path.join(params['save_path'].replace('result','model'), f'model_epoch_{epoch}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # 각 클래스별 동일 개수로 label 생성\n",
    "    num_per_class = params['batch_size'] // len(class_list)\n",
    "    label_list = []\n",
    "    for i in range(len(class_list)):\n",
    "        label_list.extend([i] * num_per_class)\n",
    "    label_tensor = torch.tensor(label_list, device=device)\n",
    "    class_onehot = torch.nn.functional.one_hot(label_tensor, num_classes=len(class_list)).float()\n",
    "\n",
    "    # 입력 noise 생성\n",
    "    z = torch.randn(len(label_tensor), params['inch'], params['image_size'], params['image_size']).to(device)\n",
    "    z = z * shared_sigmas[0].view(1, 1, 1, 1)\n",
    "\n",
    "    # 샘플링 루프: Euler-style\n",
    "    sigma = torch.full((z.shape[0], 1, 1, 1), params['sigma_max'], device=device)\n",
    "    for _ in tqdm(range(num_steps-1), desc=\"Sampling\"):\n",
    "        sigma = shared_sigmas[i].view(1, 1, 1, 1)\n",
    "        sigma_next = shared_sigmas[i + 1].view(1, 1, 1, 1)\n",
    "        denoised = model(z, sigma, class_labels=class_onehot)\n",
    "        d = (z - denoised) / sigma\n",
    "        dt = sigma_next - sigma\n",
    "        z = z + d * dt\n",
    "        sigma = sigma + dt\n",
    "        sigma = sigma.clamp(min=params['sigma_min'])\n",
    "\n",
    "    samples = transback(z).clamp(0,1)\n",
    "    save_image(samples, os.path.join(params['save_path'], f'sample_epoch_{epoch}.png'), nrow=num_per_class,normalize=True)\n",
    "    torch.save(model.state_dict(), os.path.join(params['save_path'].replace('result','model'), f'model_epoch_{epoch}.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
