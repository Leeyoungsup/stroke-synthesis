{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7916a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from RealESRGAN import RealESRGAN\n",
    "import os\n",
    "import argparse\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import save_image\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from glob import glob\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import random\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from matplotlib import pyplot as plt\n",
    "import nibabel as nib\n",
    "from cyclegan_3d.base_model import BaseModel\n",
    "from cyclegan_3d.cycle_gan_model import CycleGANModel\n",
    "from cyclegan_3d.networks3D import define_G, define_D\n",
    "#-*- coding:utf-8 -*-\n",
    "from med_ddpm.diffusion_model.trainer import GaussianDiffusion, num_to_groups\n",
    "from med_ddpm.diffusion_model.trainer import GaussianDiffusion, Trainer\n",
    "from med_ddpm.diffusion_model.unet import create_model\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from med_ddpm.utils.dtypes import LabelEnum\n",
    "import torchio as tio\n",
    "\n",
    "\n",
    "print(f\"GPUs used:\\t{torch.cuda.device_count()}\")\n",
    "device = torch.device(\"cuda\",1)\n",
    "print(f\"Device:\\t\\t{device}\")\n",
    "import pytorch_model_summary as tms\n",
    "import random\n",
    "import cv2\n",
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ea218d",
   "metadata": {},
   "source": [
    "\n",
    "<font size=\"6\">lschemic_CT & SuperResolution</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f2665f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"with_condition\": True,\n",
    "    \"inputfolder\": \"../../data/registration_data/registration_DWI/\",\n",
    "    \"batchsize\": 1,\n",
    "    \"epochs\": 10000,\n",
    "    \"input_size\": 128,\n",
    "    \"depth_size\": 64,\n",
    "    \"num_channels\": 64,\n",
    "    \"num_res_blocks\": 1,\n",
    "    \"timesteps\": 250,\n",
    "    \"save_and_sample_every\": 10,\n",
    "    \"model_save_path\": \"../../model/med_ddpm_translation/dwi2ct/\",\n",
    "    \"resume_weight\": \"../../model/med_ddpm/translation/dwi2ct.pt\"\n",
    "}\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device: \", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b7a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sample(diffusion, sample_shape, device,temp, condition_tensors=None):\n",
    "    diffusion.eval()\n",
    "    transform = Compose([\n",
    "    Lambda(lambda t: torch.tensor(t).float()),\n",
    "    Lambda(lambda t: t.unsqueeze(0))\n",
    "        ])\n",
    "    batchsize=8\n",
    "    # 샘플 생성\n",
    "    condition_tensors1= transform(condition_tensors.get_fdata()).unsqueeze(0).repeat(batchsize, 1, 1, 1, 1)\n",
    "    max_file_size_kb = 3072\n",
    "    file_saved = False\n",
    "    for j in range(1):\n",
    "        samples = diffusion.sample(batch_size=batchsize, condition_tensors=condition_tensors1.to(device))\n",
    "        samples = samples.cpu()\n",
    "        for i in range(batchsize):\n",
    "            temp_img=samples[i][0].numpy()\n",
    "            nifti_img = nib.Nifti1Image(temp_img, affine=np.eye(4))\n",
    "            nib.save(nifti_img, temp)\n",
    "            file_size_kb = os.path.getsize(temp) / 1024\n",
    "            if file_size_kb <= max_file_size_kb:\n",
    "                    file_saved = True\n",
    "                    saved_count += 1\n",
    "                    break\n",
    "            else:\n",
    "                os.remove(temp)  # 너무 크면 삭제하고 재시도\n",
    "                continue\n",
    "        if file_saved:\n",
    "            break\n",
    "    return temp_img\n",
    "\n",
    "def to_numpy_img(tensor):\n",
    "    tensor = tensor.detach().cpu().clamp(0, 1)  # ensure valid range\n",
    "    return tensor.permute(1, 2, 0).numpy()\n",
    "tf=transforms.ToTensor()\n",
    "topilimage = torchvision.transforms.ToPILImage()\n",
    "dwi_sr_model = RealESRGAN(device, scale=2).model.to(device)\n",
    "dwi_sr_model.load_state_dict(torch.load('../../model/ESRGAN/SWI/ckpt_45_checkpoint.pt',map_location=device))\n",
    "ct_sr_model = RealESRGAN(device, scale=2).model.to(device)\n",
    "ct_sr_model.load_state_dict(torch.load('../../model/ESRGAN/CT/ckpt_1000_checkpoint.pt',map_location=device))\n",
    "tr_model = create_model(params[\"input_size\"], params[\"num_channels\"], params[\"num_res_blocks\"], in_channels=2, out_channels=1).to(device)\n",
    "diffusion = GaussianDiffusion(\n",
    "    tr_model,\n",
    "    image_size = params[\"input_size\"],\n",
    "    depth_size = params[\"depth_size\"],\n",
    "    timesteps = 250,   # number of steps\n",
    "    loss_type = 'L1', \n",
    "    with_condition=True,\n",
    ").to(device)\n",
    "diffusion.load_state_dict(torch.load(params[\"resume_weight\"],map_location=device))\n",
    "print(\"Model Loaded!\")\n",
    "diffusion.eval()\n",
    "from types import SimpleNamespace\n",
    "\n",
    "data_path='../../result/generator/Ischemic_DWI/'\n",
    "save_path='../../result/generator_sr/'\n",
    "nii_list=glob(data_path+'*.nii.gz')\n",
    "nii_mask_list=[f.replace('/Ischemic_DWI','/Ischemic_mask') for f in nii_list]\n",
    "step=0\n",
    "for i in tqdm(range(len(nii_list))):\n",
    "    nii_img=nib.load(nii_list[i])\n",
    "    nii_img_data=(nii_img.get_fdata()+1.)/2.\n",
    "    nii_img_mask=nib.load(nii_mask_list[i])\n",
    "    nii_img_mask_data=nii_img_mask.get_fdata()\n",
    "    step+=1\n",
    "    folder_name=str(step).zfill(6)\n",
    "    tensor_A=tf(nii_img.get_fdata()).float().to(device)\n",
    "    a=tensor_A.permute(1, 2, 0).unsqueeze(0).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        fake_CT = save_sample(diffusion=diffusion,\n",
    "            sample_shape=(1, diffusion.channels, diffusion.depth_size, diffusion.image_size, diffusion.image_size),\n",
    "            device=device,\n",
    "            condition_tensors=nii_img,\n",
    "            temp='../../result/generator_sr/temp/dwi2ct.nii.gz')  # Clamp to 0~1 range\n",
    "        fake_CT=(fake_CT+1.)/2\n",
    "        fake_CT=fake_CT.squeeze()\n",
    "    dwi_slide=[]\n",
    "    ct_slide=[]\n",
    "    mask_slide=[]\n",
    "    for j in range(len(nii_img_data)):  # len(nii_img_data)\n",
    "        image=tf(topilimage(nii_img_data[j]).convert('RGB')).float().to(device)\n",
    "        image=image.unsqueeze(0)\n",
    "        ct_image=tf(topilimage(fake_CT[j]).convert('RGB')).float().to(device).unsqueeze(0)\n",
    "        sr_mask=cv2.resize(nii_img_mask_data[j],(256,256))\n",
    "        sr_mask=np.where(sr_mask==2,1,0)*255\n",
    "    \n",
    "        mask_slide.append(sr_mask)\n",
    "        with torch.no_grad():\n",
    "            sr = dwi_sr_model(image)\n",
    "            sr = sr.squeeze(0).cpu().numpy()\n",
    "            sr_ct=ct_sr_model(ct_image)\n",
    "            sr_ct = sr_ct.squeeze(0).cpu().numpy()\n",
    "            dwi_slide.append(sr[0])\n",
    "            ct_slide.append(sr_ct[0])\n",
    "        \n",
    "    dwi_slide=np.array(dwi_slide)\n",
    "    # min-max 정규화\n",
    "    dwi_slide = (dwi_slide - dwi_slide.min()) / (dwi_slide.max() - dwi_slide.min())\n",
    "    dwi_slide=dwi_slide*255\n",
    "    dwi_slide = dwi_slide.astype(np.uint8)\n",
    "    ct_slide=np.array(ct_slide)\n",
    "\n",
    "    ct_slide = (ct_slide - ct_slide.min()) / (ct_slide.max() - ct_slide.min())\n",
    "    ct_slide=ct_slide*255\n",
    "    ct_slide = ct_slide.astype(np.uint8)\n",
    "    mask_slide=np.array(mask_slide).astype(np.uint8)\n",
    "    create_dir(save_path+'Ischemic_DWI/'+folder_name)\n",
    "    create_dir(save_path+'Ischemic_CT/'+folder_name)\n",
    "    create_dir(save_path+'Ischemic_mask/'+folder_name)\n",
    "    for j in range(len(nii_img_data)):\n",
    "        Image.fromarray(dwi_slide[j]).save(save_path+'Ischemic_DWI/'+folder_name+'/'+str(j).zfill(6)+'.png')\n",
    "        Image.fromarray(ct_slide[j]).save(save_path+'Ischemic_CT/'+folder_name+'/'+str(j).zfill(6)+'.png')\n",
    "        Image.fromarray(mask_slide[j]).save(save_path+'Ischemic_mask/'+folder_name+'/'+str(j).zfill(6)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a3565",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_CT.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05e2e90",
   "metadata": {},
   "source": [
    "<font size=\"6\">Normal CT & SuperResolution</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623eb95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy_img(tensor):\n",
    "    tensor = tensor.detach().cpu().clamp(0, 1)  # ensure valid range\n",
    "    return tensor.permute(1, 2, 0).numpy()\n",
    "tf=transforms.ToTensor()\n",
    "topilimage = torchvision.transforms.ToPILImage()\n",
    "dwi_sr_model = RealESRGAN(device, scale=2).model.to(device)\n",
    "dwi_sr_model.load_state_dict(torch.load('../../model/ESRGAN/SWI/ckpt_45_checkpoint.pt',map_location=device))\n",
    "ct_sr_model = RealESRGAN(device, scale=2).model.to(device)\n",
    "ct_sr_model.load_state_dict(torch.load('../../model/ESRGAN/CT/ckpt_1000_checkpoint.pt',map_location=device))\n",
    "\n",
    "from types import SimpleNamespace\n",
    "opt = SimpleNamespace(**params)\n",
    "# tr_model = CycleGANModel()\n",
    "# tr_model.initialize(opt)\n",
    "# tr_model.setup(opt)\n",
    "# tr_model.device = device\n",
    "# tr_model .load_networks(153)\n",
    "data_path='../../result/generator/Normal_DWI/'\n",
    "save_path='../../result/generator_sr/'\n",
    "nii_list=glob(data_path+'*.nii.gz')\n",
    "nii_mask_list=[f.replace('/Normal_DWI','/Normal_mask') for f in nii_list]\n",
    "step=0\n",
    "for i in tqdm(range(len(nii_list))):\n",
    "    nii_img=nib.load(nii_list[i])\n",
    "    nii_img_data=(nii_img.get_fdata()+1.)/2.\n",
    "    nii_img_mask=nib.load(nii_mask_list[i])\n",
    "    nii_img_mask_data=nii_img_mask.get_fdata()\n",
    "    step+=1\n",
    "    folder_name=str(step).zfill(6)\n",
    "    tensor_A=tf(nii_img.get_fdata()).float().to(device)\n",
    "    a=tensor_A .permute(1, 0, 2).unsqueeze(0).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        fake_CT = tr_model.netG_A(a)# Clamp to 0~1 range\n",
    "        fake_CT=torch.where(fake_CT<-1,-1,fake_CT)\n",
    "        fake_CT=torch.where(fake_CT>1,1.,fake_CT)\n",
    "        fake_CT=(fake_CT+1.)/2\n",
    "        fake_CT=fake_CT.squeeze()\n",
    "    dwi_slide=[]\n",
    "    ct_slide=[]\n",
    "    mask_slide=[]\n",
    "    for j in range(len(nii_img_data)):  # len(nii_img_data)\n",
    "        image=tf(topilimage(nii_img_data[j]).convert('RGB')).float().to(device)\n",
    "        image=image.unsqueeze(0)\n",
    "        ct_image=tf(topilimage(fake_CT[j].squeeze().cpu()).convert('RGB')).float().to(device).unsqueeze(0)\n",
    "        sr_mask=cv2.resize(nii_img_mask_data[j],(256,256))\n",
    "        sr_mask=np.where(sr_mask==2,1,0)*255\n",
    "    \n",
    "        mask_slide.append(sr_mask)\n",
    "        with torch.no_grad():\n",
    "            sr = dwi_sr_model(image)\n",
    "            sr = sr.squeeze(0).cpu().numpy()\n",
    "            sr_ct=ct_sr_model(ct_image)\n",
    "            sr_ct = sr_ct.squeeze(0).cpu().numpy()\n",
    "            dwi_slide.append(sr[0])\n",
    "            ct_slide.append(sr_ct[0])\n",
    "        \n",
    "    dwi_slide=np.array(dwi_slide)\n",
    "    # min-max 정규화\n",
    "    dwi_slide = (dwi_slide - dwi_slide.min()) / (dwi_slide.max() - dwi_slide.min())\n",
    "    dwi_slide=dwi_slide*255\n",
    "    dwi_slide = dwi_slide.astype(np.uint8)\n",
    "    ct_slide=np.array(ct_slide)\n",
    "\n",
    "    ct_slide = (ct_slide - ct_slide.min()) / (ct_slide.max() - ct_slide.min())\n",
    "    ct_slide=ct_slide*255\n",
    "    ct_slide = ct_slide.astype(np.uint8)\n",
    "    mask_slide=np.array(mask_slide).astype(np.uint8)\n",
    "    create_dir(save_path+'Normal_DWI/'+folder_name)\n",
    "    create_dir(save_path+'Normal_CT/'+folder_name)\n",
    "    create_dir(save_path+'Normal_mask/'+folder_name)\n",
    "    for j in range(len(nii_img_data)):\n",
    "        Image.fromarray(dwi_slide[j]).save(save_path+'Normal_DWI/'+folder_name+'/'+str(j).zfill(6)+'.png')\n",
    "        Image.fromarray(ct_slide[j]).save(save_path+'Normal_CT/'+folder_name+'/'+str(j).zfill(6)+'.png')\n",
    "        Image.fromarray(mask_slide[j]).save(save_path+'Normal_mask/'+folder_name+'/'+str(j).zfill(6)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c295c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary(tr_model.netG_A, input_size=(1, 1, 64, 128, 128), device=opt.device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
