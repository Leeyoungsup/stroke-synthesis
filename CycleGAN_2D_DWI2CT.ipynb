{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image\n",
    "import itertools\n",
    "import random\n",
    "import nibabel as nib\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "topilimage =transforms.ToPILImage()\n",
    "device = torch.device(\"cuda:4\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model params\n",
    "params = {\n",
    "    'batch_size':1,              # 128x128이면 그대로 사용 가능 (GPU 메모리에 따라 128로 늘려도 OK)\n",
    "    'input_size':128,\n",
    "    'resize_scale':128,          # resize도 128로 고정\n",
    "    'crop_size':128,             # crop도 동일하게\n",
    "    'fliplr':False,              # 필요에 따라 True로 변경 가능 (augmentation 목적)\n",
    "    'num_epochs':500,            # 이미지 작아졌으니 500 정도로 줄여도 무방\n",
    "    'decay_epoch':25,            # 절반 시점에 decay\n",
    "\n",
    "    'ngf':16,                    # generator filter 수 절반으로 축소\n",
    "    'ndf':32,                    # discriminator filter 수도 축소\n",
    "    'num_resnet':3,              # resnet block 수도 줄이기 (128에서는 3~4 추천)\n",
    "    \n",
    "    'lrG':2e-4,                  # 이미지가 작아졌기 때문에 learning rate는 살짝 키워도 안정적 (2e-4)\n",
    "    'lrD':2e-4,                  #\n",
    "    'beta1':0.5,\n",
    "    'beta2':0.999,\n",
    "    \n",
    "    'lambdaA':10,\n",
    "    'lambdaB':10,\n",
    "    'img_form':'nii.gz'\n",
    "}\n",
    "\n",
    "data_dir = '../../data/registration_data/'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np(x):\n",
    "    return x.data.cpu().numpy()\n",
    "class ImagePool():\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "        if self.pool_size > 0:\n",
    "            self.num_imgs = 0\n",
    "            self.images = []\n",
    "\n",
    "    def query(self, images):\n",
    "        if self.pool_size == 0:\n",
    "            return images\n",
    "        return_images = []\n",
    "        with torch.no_grad():  # No need to track gradients\n",
    "            for image in images:\n",
    "                image = torch.unsqueeze(image, 0)\n",
    "                if self.num_imgs < self.pool_size:\n",
    "                    self.num_imgs += 1\n",
    "                    self.images.append(image)\n",
    "                    return_images.append(image)\n",
    "                else:\n",
    "                    p = random.uniform(0, 1)\n",
    "                    if p > 0.5:\n",
    "                        random_id = random.randint(0, self.pool_size - 1)\n",
    "                        tmp = self.images[random_id].clone()\n",
    "                        self.images[random_id] = image\n",
    "                        return_images.append(tmp)\n",
    "                    else:\n",
    "                        return_images.append(image)\n",
    "            return_images = torch.cat(return_images, 0)\n",
    "        return return_images.detach()  \n",
    "        \n",
    "class DatasetFromFolder(data.Dataset):\n",
    "    def __init__(self, mri_image_list,ct_image_list):\n",
    "        super(DatasetFromFolder, self).__init__()\n",
    "        self.mri_image_list =mri_image_list\n",
    "        self.ct_image_list =ct_image_list\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load Image\n",
    "        img = self.mri_image_list[index]\n",
    "        target = self.ct_image_list[index]\n",
    "        return img, target\n",
    "    def __len__(self):\n",
    "        return len(self.mri_image_list)\n",
    "    \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=params['input_size']),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "data_a_list=glob(data_dir+'registration_DWI/*.'+params['img_form'])\n",
    "data_b_list=[f.replace('/registration_DWI', '/CT') for f in data_a_list]\n",
    "MRI_img_tensor=torch.zeros(len(data_a_list)*40,1,params['input_size'],params['input_size'])\n",
    "CT_img_tensor=torch.zeros(len(data_b_list)*40,1,params['input_size'],params['input_size'])\n",
    "for i in tqdm(range(len(data_a_list))):\n",
    "    nib_img=nib.load(data_a_list[i])\n",
    "    mri_img=nib_img.get_fdata()\n",
    "    mri_img_tensor = torch.from_numpy(mri_img).unsqueeze(1).float()-1.  # (40, 1, 256, 256)\n",
    "\n",
    "    # Resize to (40, 1, 128, 128)\n",
    "    img_tensor_resized = F.interpolate(mri_img_tensor, size=(128, 128), mode='bilinear', align_corners=False)\n",
    "\n",
    "    MRI_img_tensor[i*40:(i+1)*40] = img_tensor_resized\n",
    "\n",
    "    ct_img=nib.load(data_b_list[i])\n",
    "    ct_img=ct_img.get_fdata()\n",
    "    ct_img_tensor = torch.from_numpy(ct_img).unsqueeze(1).float()-1.  # (40, 1, 256, 256)\n",
    "    # Resize to (40, 1, 128, 128)\n",
    "    img_tensor_resized = F.interpolate(ct_img_tensor, size=(128, 128), mode='bilinear', align_corners=False)\n",
    "    CT_img_tensor[i*40:(i+1)*40] = img_tensor_resized\n",
    "train_data_A = DatasetFromFolder(MRI_img_tensor,CT_img_tensor)\n",
    "loader = torch.utils.data.DataLoader(dataset=train_data_A , batch_size=params['batch_size'], shuffle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CycleGAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(features, features, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(features, features, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm2d(features),\n",
    "            nn.Dropout(0.5)  # Dropout 추가 (드롭아웃 확률 0.5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "# Generator Model\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_channels, output_channels, n_residual_blocks=9):\n",
    "        super(Generator, self).__init__()\n",
    "        # 초기 컨볼루션 블록\n",
    "        model = [\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=7, padding=3),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "\n",
    "        # 다운샘플링\n",
    "        in_features = 64\n",
    "        out_features = in_features * 2\n",
    "        for _ in range(4):  # 기존 2에서 4로 변경\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "\n",
    "        # 잔차 블록\n",
    "        for _ in range(n_residual_blocks):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "\n",
    "        # 업샘플링\n",
    "        out_features = in_features // 2\n",
    "        for _ in range(4):  # 기존 2에서 4로 변경\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features // 2\n",
    "\n",
    "        # 출력 레이어\n",
    "        model += [nn.Conv2d(64, output_channels, kernel_size=7, padding=3), nn.Tanh()]\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Discriminator Model\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        def discriminator_block(in_filters, out_filters, normalization=True):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, kernel_size=4, stride=2, padding=1)]\n",
    "            if normalization:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(input_channels, 64, normalization=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(1, 1).to(device)  # 그레이스케일에서 컬러로\n",
    "F = Generator(1, 1).to(device)  # 컬러에서 그레이스케일로\n",
    "D_ct = Discriminator(1).to(device)\n",
    "D_mri = Discriminator(1).to(device)\n",
    "\n",
    "# 옵티마이저 설정\n",
    "optimizer_G = optim.Adam(itertools.chain(G.parameters(), F.parameters()), lr=2e-5, betas=(0.5, 0.999))\n",
    "optimizer_D_ct = optim.Adam(D_ct.parameters(), lr=2e-5, betas=(0.5, 0.999))\n",
    "optimizer_D_mri = optim.Adam(D_mri.parameters(), lr=2e-5, betas=(0.5, 0.999))\n",
    "\n",
    "# 학습률 스케줄러 추가\n",
    "lr_scheduler_G = optim.lr_scheduler.StepLR(optimizer_G, step_size=20, gamma=0.5)\n",
    "lr_scheduler_D_ct = optim.lr_scheduler.StepLR(optimizer_D_ct, step_size=20, gamma=0.5)\n",
    "lr_scheduler_D_mri = optim.lr_scheduler.StepLR(optimizer_D_mri, step_size=20, gamma=0.5)\n",
    "\n",
    "# 손실 함수 설정\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/500:  49%|████▉     | 2446/4950 [04:06<04:10, 10.00it/s, Loss G=1.6852, Loss D_ct=0.2266, Loss D_mri=0.2151]"
     ]
    }
   ],
   "source": [
    "for epoch in range(params['num_epochs']):\n",
    "    \n",
    "    total_loss_G=0\n",
    "    total_loss_D_ct=0\n",
    "    total_loss_D_mri=0\n",
    "    count=0\n",
    "    with tqdm(loader, total=len(loader), desc=f\"Epoch {epoch+1}/{params['num_epochs']}\") as pbar:\n",
    "        for mri_img,ct_img in pbar:\n",
    "            mri_img = mri_img.to(device)\n",
    "            ct_img = ct_img.to(device)\n",
    "\n",
    "            # 생성자 G와 F 업데이트\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # 아이덴티티 손실\n",
    "            loss_id_G = criterion_identity(G(ct_img), ct_img) * 5.0\n",
    "            loss_id_F = criterion_identity(F(mri_img), mri_img) * 5.0\n",
    "\n",
    "            # GAN 손실\n",
    "            fake_ct = G(mri_img)\n",
    "            pred_fake = D_ct(fake_ct)\n",
    "            loss_GAN_G = criterion_GAN(pred_fake, torch.ones_like(pred_fake).to(device))\n",
    "\n",
    "            fake_mri = F(ct_img)\n",
    "            pred_fake = D_mri(fake_mri)\n",
    "            loss_GAN_F = criterion_GAN(pred_fake, torch.ones_like(pred_fake).to(device))\n",
    "\n",
    "            # 순환 일관성 손실\n",
    "            recov_mri = F(fake_ct)\n",
    "            loss_cycle_GF = criterion_cycle(recov_mri, mri_img) * 10.0\n",
    "\n",
    "            recov_ct = G(fake_mri)\n",
    "            loss_cycle_FG = criterion_cycle(recov_ct, ct_img) * 10.0\n",
    "\n",
    "            # 총 생성자 손실\n",
    "            loss_G = loss_id_G + loss_id_F + loss_GAN_G + loss_GAN_F + loss_cycle_GF + loss_cycle_FG\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # 판별자 D_ct 업데이트\n",
    "            optimizer_D_ct.zero_grad()\n",
    "\n",
    "            pred_real = D_ct(ct_img)\n",
    "            loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real).to(device))\n",
    "\n",
    "            pred_fake = D_ct(fake_ct.detach())\n",
    "            loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake).to(device))\n",
    "\n",
    "            loss_D_ct = (loss_D_real + loss_D_fake) * 0.5\n",
    "            loss_D_ct.backward()\n",
    "            optimizer_D_ct.step()\n",
    "\n",
    "            # 판별자 D_mri 업데이트\n",
    "            optimizer_D_mri.zero_grad()\n",
    "\n",
    "            pred_real = D_mri(mri_img)\n",
    "            loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real).to(device))\n",
    "\n",
    "            pred_fake = D_mri(fake_mri.detach())\n",
    "            loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake).to(device))\n",
    "\n",
    "            loss_D_mri = (loss_D_real + loss_D_fake) * 0.5\n",
    "            loss_D_mri.backward()\n",
    "            optimizer_D_mri.step()\n",
    "            with torch.no_grad():\n",
    "                fake_ct = G(mri_img)\n",
    "                recov_mri = F(fake_ct)\n",
    "            def denormalize(img):\n",
    "                return img * 0.5 + 0.5\n",
    "\n",
    "            mri_img_vis = denormalize(mri_img[0])\n",
    "            ct_img_vis = denormalize(ct_img[0])\n",
    "            fake_ct_vis = denormalize(fake_ct[0])\n",
    "            recov_mri_vis = denormalize(recov_mri[0])\n",
    "\n",
    "            # 이미지 리스트 생성\n",
    "            images = [mri_img_vis, fake_ct_vis, recov_mri_vis, ct_img_vis]\n",
    "\n",
    "            # 이미지들을 너비 방향으로 concatenate\n",
    "            concatenated = torch.cat(images, dim=2)  # dim=3은 너비 방향\n",
    "            total_loss_D_ct+=loss_D_ct.item()\n",
    "            total_loss_D_mri+=loss_D_mri.item()\n",
    "            total_loss_G+=loss_G.item()\n",
    "            count+=1\n",
    "            pbar.set_postfix({\n",
    "                'Loss G': f'{total_loss_G/count:.4f}',\n",
    "                'Loss D_ct': f'{total_loss_D_ct/count:.4f}',\n",
    "                'Loss D_mri': f'{total_loss_D_mri/count:.4f}'\n",
    "            })\n",
    "\n",
    "    save_image(concatenated, f'../../result/translation/mri2ct/concatenated_epoch{epoch}.png')\n",
    "    torch.save(G.state_dict(), f'../../model/translation/mri2ct/G_{epoch}.pth')\n",
    "    torch.save(F.state_dict(), f'../../model/translation/mri2ct/F_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 로드\n",
    "G.load_state_dict(torch.load('G.pth'))\n",
    "G.eval()\n",
    "\n",
    "# 테스트 이미지 로드\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "from PIL import Image\n",
    "image = Image.open('path_to_grayscale_image.jpg')\n",
    "image = test_transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "# 컬러화된 이미지 생성\n",
    "with torch.no_grad():\n",
    "    fake_color = G(image)\n",
    "\n",
    "# 이미지 저장\n",
    "save_image(fake_color, 'colorized_image.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
